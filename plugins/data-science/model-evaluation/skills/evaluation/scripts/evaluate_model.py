#!/usr/bin/env python3
"""
ëª¨ë¸ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

í•™ìŠµëœ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•˜ê³  ì‹œê°í™”í•©ë‹ˆë‹¤.

ì„¤ì¹˜:
    cd plugins/model-evaluation/skills/evaluation
    uv pip install -r requirements.txt

ì‚¬ìš©ë²•:
    python evaluate_model.py --model-path "./models/model.pkl" --test-data "./data/test.csv" --target-column "Class"
    python evaluate_model.py --model-path "./models/model.pkl" --test-data "./data/test.csv" --task-type classification

í•„ìš” íŒ¨í‚¤ì§€:
    - pandas
    - scikit-learn
    - matplotlib
    - seaborn
    - joblib
"""

import argparse
import os
import sys
from pathlib import Path

import joblib
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
    f1_score,
    mean_absolute_error,
    mean_squared_error,
    precision_recall_curve,
    precision_score,
    r2_score,
    recall_score,
    roc_auc_score,
    roc_curve,
)
from sklearn.model_selection import cross_val_score, learning_curve


def print_header(text):
    """í—¤ë” ì¶œë ¥"""
    print(f"\n{'=' * 60}")
    print(text)
    print('=' * 60)


def print_section(text):
    """ì„¹ì…˜ ì¶œë ¥"""
    print(f"\n{'-' * 60}")
    print(text)
    print('-' * 60)


def load_data(data_path, target_column):
    """ë°ì´í„° ë¡œë“œ"""
    print(f"\nâœ“ ë°ì´í„° ë¡œë“œ ì¤‘: {data_path}")

    file_ext = Path(data_path).suffix.lower()

    if file_ext == '.csv':
        df = pd.read_csv(data_path)
    elif file_ext in ['.xlsx', '.xls']:
        df = pd.read_excel(data_path)
    elif file_ext == '.parquet':
        df = pd.read_parquet(data_path)
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_ext}")

    if target_column not in df.columns:
        raise ValueError(f"íƒ€ê²Ÿ ì»¬ëŸ¼ '{target_column}'ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤.")

    X = df.drop(columns=[target_column])
    y = df[target_column]

    print(f"âœ“ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df):,}ê±´, {len(X.columns)}ê°œ íŠ¹ì„±")

    return X, y, df


def load_model(model_path):
    """ëª¨ë¸ ë¡œë“œ"""
    print(f"\nâœ“ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_path}")

    if not os.path.exists(model_path):
        raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")

    model = joblib.load(model_path)
    print(f"âœ“ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {type(model).__name__}")

    return model


def plot_feature_importance(model, feature_names, output_dir, top_n=20):
    """íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”"""
    print_section("íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„")

    # íŠ¹ì„± ì¤‘ìš”ë„ ì¶”ì¶œ
    if hasattr(model, 'feature_importances_'):
        importance = model.feature_importances_
    elif hasattr(model, 'coef_'):
        importance = np.abs(model.coef_).flatten()
    else:
        print("âš ï¸  ì´ ëª¨ë¸ì€ íŠ¹ì„± ì¤‘ìš”ë„ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return

    # ìƒìœ„ Nê°œ íŠ¹ì„±
    indices = np.argsort(importance)[::-1][:top_n]
    top_features = [feature_names[i] for i in indices]
    top_importance = importance[indices]

    # ì‹œê°í™”
    plt.figure(figsize=(10, 8))
    plt.barh(range(len(top_features)), top_importance, align='center')
    plt.yticks(range(len(top_features)), top_features)
    plt.xlabel('Importance')
    plt.title(f'Top {top_n} Feature Importance')
    plt.gca().invert_yaxis()
    plt.tight_layout()

    output_path = os.path.join(output_dir, 'feature_importance.png')
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()

    print(f"âœ“ íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™” ì €ì¥: {output_path}")

    # ì½˜ì†” ì¶œë ¥
    print(f"\nìƒìœ„ {min(10, len(top_features))}ê°œ ì¤‘ìš” íŠ¹ì„±:")
    for i, (feat, imp) in enumerate(zip(top_features[:10], top_importance[:10]), 1):
        print(f"  {i:2d}. {feat:30s}: {imp:.4f}")


def plot_learning_curves(model, X, y, output_dir, cv=5):
    """í•™ìŠµ ê³¡ì„  ì‹œê°í™”"""
    print_section("í•™ìŠµ ê³¡ì„  ë¶„ì„")

    print("â³ í•™ìŠµ ê³¡ì„  ê³„ì‚° ì¤‘ (ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)...")

    train_sizes, train_scores, val_scores = learning_curve(
        model, X, y, cv=cv, n_jobs=-1,
        train_sizes=np.linspace(0.1, 1.0, 10),
        scoring='f1' if hasattr(model, 'predict_proba') else 'r2'
    )

    train_mean = np.mean(train_scores, axis=1)
    train_std = np.std(train_scores, axis=1)
    val_mean = np.mean(val_scores, axis=1)
    val_std = np.std(val_scores, axis=1)

    plt.figure(figsize=(10, 6))
    plt.plot(train_sizes, train_mean, label='Training score', marker='o')
    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)
    plt.plot(train_sizes, val_mean, label='Validation score', marker='s')
    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1)

    plt.xlabel('Training Set Size')
    plt.ylabel('Score')
    plt.title('Learning Curves')
    plt.legend(loc='best')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()

    output_path = os.path.join(output_dir, 'learning_curves.png')
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()

    print(f"âœ“ í•™ìŠµ ê³¡ì„  ì €ì¥: {output_path}")
    print(f"  ìµœì¢… í•™ìŠµ ìŠ¤ì½”ì–´: {train_mean[-1]:.4f} (Â±{train_std[-1]:.4f})")
    print(f"  ìµœì¢… ê²€ì¦ ìŠ¤ì½”ì–´: {val_mean[-1]:.4f} (Â±{val_std[-1]:.4f})")


def plot_confusion_matrix(y_true, y_pred, output_dir):
    """í˜¼ë™ í–‰ë ¬ ì‹œê°í™”"""
    cm = confusion_matrix(y_true, y_pred)

    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Confusion Matrix')
    plt.tight_layout()

    output_path = os.path.join(output_dir, 'confusion_matrix.png')
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()

    print(f"âœ“ í˜¼ë™ í–‰ë ¬ ì €ì¥: {output_path}")


def plot_roc_curve(y_true, y_pred_proba, output_dir):
    """ROC ê³¡ì„  ì‹œê°í™”"""
    fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)
    roc_auc = roc_auc_score(y_true, y_pred_proba)

    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})', linewidth=2)
    plt.plot([0, 1], [0, 1], 'k--', label='Random', linewidth=1)
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend(loc='lower right')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()

    output_path = os.path.join(output_dir, 'roc_curve.png')
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()

    print(f"âœ“ ROC ê³¡ì„  ì €ì¥: {output_path}")
    print(f"  ROC AUC: {roc_auc:.4f}")


def plot_precision_recall_curve(y_true, y_pred_proba, output_dir):
    """Precision-Recall ê³¡ì„  ì‹œê°í™”"""
    precision, recall, thresholds = precision_recall_curve(y_true, y_pred_proba)

    plt.figure(figsize=(8, 6))
    plt.plot(recall, precision, linewidth=2)
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curve')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()

    output_path = os.path.join(output_dir, 'precision_recall_curve.png')
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()

    print(f"âœ“ Precision-Recall ê³¡ì„  ì €ì¥: {output_path}")


def evaluate_classification(model, X, y, output_dir):
    """ë¶„ë¥˜ ëª¨ë¸ í‰ê°€"""
    print_section("ë¶„ë¥˜ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€")

    # ì˜ˆì¸¡
    y_pred = model.predict(X)

    # ê¸°ë³¸ ë©”íŠ¸ë¦­
    accuracy = accuracy_score(y, y_pred)
    precision = precision_score(y, y_pred, average='weighted', zero_division=0)
    recall = recall_score(y, y_pred, average='weighted', zero_division=0)
    f1 = f1_score(y, y_pred, average='weighted', zero_division=0)

    print(f"\nê¸°ë³¸ ë©”íŠ¸ë¦­:")
    print(f"  Accuracy:  {accuracy:.4f}")
    print(f"  Precision: {precision:.4f}")
    print(f"  Recall:    {recall:.4f}")
    print(f"  F1-Score:  {f1:.4f}")

    # Classification Report
    print(f"\nìƒì„¸ ë¦¬í¬íŠ¸:")
    print(classification_report(y, y_pred))

    # í˜¼ë™ í–‰ë ¬
    plot_confusion_matrix(y, y_pred, output_dir)

    # ROC ê³¡ì„  (ì´ì§„ ë¶„ë¥˜ì¸ ê²½ìš°)
    if hasattr(model, 'predict_proba') and len(np.unique(y)) == 2:
        y_pred_proba = model.predict_proba(X)[:, 1]
        plot_roc_curve(y, y_pred_proba, output_dir)
        plot_precision_recall_curve(y, y_pred_proba, output_dir)

    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1
    }


def evaluate_regression(model, X, y, output_dir):
    """íšŒê·€ ëª¨ë¸ í‰ê°€"""
    print_section("íšŒê·€ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€")

    # ì˜ˆì¸¡
    y_pred = model.predict(X)

    # ê¸°ë³¸ ë©”íŠ¸ë¦­
    mae = mean_absolute_error(y, y_pred)
    mse = mean_squared_error(y, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y, y_pred)

    print(f"\nê¸°ë³¸ ë©”íŠ¸ë¦­:")
    print(f"  MAE:  {mae:.4f}")
    print(f"  MSE:  {mse:.4f}")
    print(f"  RMSE: {rmse:.4f}")
    print(f"  RÂ²:   {r2:.4f}")

    # ì˜ˆì¸¡ vs ì‹¤ì œ ì‹œê°í™”
    plt.figure(figsize=(10, 6))
    plt.scatter(y, y_pred, alpha=0.5)
    plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', linewidth=2)
    plt.xlabel('Actual Values')
    plt.ylabel('Predicted Values')
    plt.title('Actual vs Predicted')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()

    output_path = os.path.join(output_dir, 'actual_vs_predicted.png')
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()

    print(f"âœ“ Actual vs Predicted ì €ì¥: {output_path}")

    # ì”ì°¨ í”Œë¡¯
    residuals = y - y_pred
    plt.figure(figsize=(10, 6))
    plt.scatter(y_pred, residuals, alpha=0.5)
    plt.axhline(y=0, color='r', linestyle='--', linewidth=2)
    plt.xlabel('Predicted Values')
    plt.ylabel('Residuals')
    plt.title('Residual Plot')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()

    output_path = os.path.join(output_dir, 'residuals.png')
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()

    print(f"âœ“ ì”ì°¨ í”Œë¡¯ ì €ì¥: {output_path}")

    return {
        'mae': mae,
        'mse': mse,
        'rmse': rmse,
        'r2': r2
    }


def perform_cross_validation(model, X, y, cv=5):
    """êµì°¨ ê²€ì¦"""
    print_section("êµì°¨ ê²€ì¦")

    print(f"â³ {cv}-Fold êµì°¨ ê²€ì¦ ìˆ˜í–‰ ì¤‘...")

    # íƒœìŠ¤í¬ íƒ€ì… ì¶”ì •
    if hasattr(model, 'predict_proba'):
        scoring = 'f1_weighted'
        scoring_name = 'F1-Score (Weighted)'
    else:
        scoring = 'r2'
        scoring_name = 'RÂ²'

    scores = cross_val_score(model, X, y, cv=cv, scoring=scoring, n_jobs=-1)

    print(f"\n{scoring_name} ìŠ¤ì½”ì–´ ({cv}-Fold CV):")
    for i, score in enumerate(scores, 1):
        print(f"  Fold {i}: {score:.4f}")
    print(f"\n  í‰ê· : {scores.mean():.4f} (Â±{scores.std():.4f})")

    return scores


def save_evaluation_report(metrics, output_dir, model_name):
    """í‰ê°€ ë¦¬í¬íŠ¸ ì €ì¥"""
    report_path = os.path.join(output_dir, f"{model_name}_evaluation_report.md")

    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(f"# ëª¨ë¸ í‰ê°€ ë¦¬í¬íŠ¸: {model_name}\n\n")
        f.write(f"## ì„±ëŠ¥ ë©”íŠ¸ë¦­\n\n")

        for key, value in metrics.items():
            if isinstance(value, (int, float)):
                f.write(f"- **{key.upper()}**: {value:.4f}\n")

        f.write(f"\n## ì‹œê°í™” ê²°ê³¼\n\n")
        f.write(f"- íŠ¹ì„± ì¤‘ìš”ë„: `feature_importance.png`\n")
        f.write(f"- í•™ìŠµ ê³¡ì„ : `learning_curves.png`\n")

        if 'accuracy' in metrics:
            f.write(f"- í˜¼ë™ í–‰ë ¬: `confusion_matrix.png`\n")
            f.write(f"- ROC ê³¡ì„ : `roc_curve.png`\n")
            f.write(f"- Precision-Recall ê³¡ì„ : `precision_recall_curve.png`\n")
        else:
            f.write(f"- Actual vs Predicted: `actual_vs_predicted.png`\n")
            f.write(f"- ì”ì°¨ í”Œë¡¯: `residuals.png`\n")

    print(f"\nâœ“ í‰ê°€ ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")


def main():
    parser = argparse.ArgumentParser(description='ëª¨ë¸ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸')
    parser.add_argument('--model-path', type=str, required=True,
                        help='í•™ìŠµëœ ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (.pkl)')
    parser.add_argument('--test-data', type=str, required=True,
                        help='í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ')
    parser.add_argument('--target-column', type=str, required=True,
                        help='íƒ€ê²Ÿ ì»¬ëŸ¼ëª…')
    parser.add_argument('--task-type', type=str, choices=['classification', 'regression', 'auto'],
                        default='auto', help='íƒœìŠ¤í¬ íƒ€ì… (ê¸°ë³¸ê°’: auto)')
    parser.add_argument('--output-dir', type=str, default=None,
                        help='ì¶œë ¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--cv', type=int, default=5,
                        help='êµì°¨ ê²€ì¦ í´ë“œ ìˆ˜ (ê¸°ë³¸ê°’: 5)')

    args = parser.parse_args()

    print_header("ëª¨ë¸ í‰ê°€ ì‹œì‘")

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    if args.output_dir:
        output_dir = args.output_dir
    else:
        # projects/{project-name}/outputs/evaluations êµ¬ì¡° ì‚¬ìš©
        test_data_path = Path(args.test_data)
        if 'projects' in test_data_path.parts:
            project_idx = test_data_path.parts.index('projects')
            project_name = test_data_path.parts[project_idx + 1]
            output_dir = f"projects/{project_name}/outputs/evaluations"
        else:
            output_dir = "outputs/evaluations"

    os.makedirs(output_dir, exist_ok=True)
    print(f"âœ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")

    # ë°ì´í„° ë¡œë“œ
    X, y, df = load_data(args.test_data, args.target_column)

    # ëª¨ë¸ ë¡œë“œ
    model = load_model(args.model_path)
    model_name = Path(args.model_path).stem

    # íƒœìŠ¤í¬ íƒ€ì… ì¶”ì •
    if args.task_type == 'auto':
        if hasattr(model, 'predict_proba') or len(np.unique(y)) <= 20:
            task_type = 'classification'
        else:
            task_type = 'regression'
        print(f"\nâœ“ ìë™ íƒœìŠ¤í¬ íƒ€ì… ê°ì§€: {task_type}")
    else:
        task_type = args.task_type

    # íŠ¹ì„± ì¤‘ìš”ë„
    plot_feature_importance(model, X.columns.tolist(), output_dir, top_n=20)

    # í•™ìŠµ ê³¡ì„ 
    plot_learning_curves(model, X, y, output_dir, cv=args.cv)

    # êµì°¨ ê²€ì¦
    cv_scores = perform_cross_validation(model, X, y, cv=args.cv)

    # ëª¨ë¸ í‰ê°€
    if task_type == 'classification':
        metrics = evaluate_classification(model, X, y, output_dir)
    else:
        metrics = evaluate_regression(model, X, y, output_dir)

    # ë¦¬í¬íŠ¸ ì €ì¥
    save_evaluation_report(metrics, output_dir, model_name)

    print_header("ëª¨ë¸ í‰ê°€ ì™„ë£Œ")
    print(f"\nğŸ“ ëª¨ë“  ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_dir}/")
    print(f"   - ì‹œê°í™”: *.png")
    print(f"   - ë¦¬í¬íŠ¸: {model_name}_evaluation_report.md")

    return 0


if __name__ == '__main__':
    sys.exit(main())
