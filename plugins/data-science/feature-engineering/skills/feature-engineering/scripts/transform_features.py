#!/usr/bin/env python3
"""
íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ìŠ¤í¬ë¦½íŠ¸

ë°ì´í„° ì „ì²˜ë¦¬ ë° íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ì„ ìˆ˜í–‰í•˜ì—¬ ëª¨ë¸ í•™ìŠµ ì¤€ë¹„ ì™„ë£Œëœ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

ì„¤ì¹˜:
    # uv ì‚¬ìš© (ê¶Œìž¥)
    cd plugins/feature-engineering/skills/feature-engineering
    uv pip install --system -r requirements.txt

ì‚¬ìš©ë²•:
    python transform_features.py \
      --data-path "./data/raw/creditcard.csv" \
      --target-column "Class" \
      --time-features "hour,day,cyclical"

í•„ìš” íŒ¨í‚¤ì§€:
    - pandas
    - numpy
    - scikit-learn
    - joblib
"""

import argparse
import os
from datetime import datetime
from pathlib import Path

import joblib
import numpy as np
import pandas as pd
from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler


def load_data(data_path):
    """ë°ì´í„° ë¡œë“œ"""
    print(f"\në°ì´í„° ë¡œë“œ ì¤‘: {data_path}")
    df = pd.read_csv(data_path)
    print(f"âœ“ ì™„ë£Œ: {len(df):,}ê±´, {len(df.columns)}ê°œ ì»¬ëŸ¼")
    return df


def extract_time_features(df, time_column='Time', features=['hour', 'day', 'cyclical']):
    """ì‹œê°„ íŠ¹ì„± ì¶”ì¶œ"""
    if time_column not in df.columns:
        print(f"âš ï¸  '{time_column}' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ì‹œê°„ íŠ¹ì„± ì¶”ì¶œ ê±´ë„ˆëœë‹ˆë‹¤.")
        return df, []

    print(f"\nì‹œê°„ íŠ¹ì„± ì¶”ì¶œ ì¤‘ (ì›ë³¸: {time_column})...")
    new_features = []

    # Hour ì¶”ì¶œ (0-23)
    if 'hour' in features:
        df['Hour'] = (df[time_column] / 3600) % 24
        new_features.append('Hour')
        print(f"  âœ“ Hour (0-23) ìƒì„±")

    # Day ì¶”ì¶œ
    if 'day' in features:
        df['Day'] = (df[time_column] / 86400).astype(int)
        new_features.append('Day')
        print(f"  âœ“ Day ìƒì„±")

    # Cyclical Encoding
    if 'cyclical' in features and 'Hour' in df.columns:
        df['Hour_sin'] = np.sin(2 * np.pi * df['Hour'] / 24)
        df['Hour_cos'] = np.cos(2 * np.pi * df['Hour'] / 24)
        new_features.extend(['Hour_sin', 'Hour_cos'])
        print(f"  âœ“ Hour_sin, Hour_cos (ì£¼ê¸°ì„± ì¸ì½”ë”©) ìƒì„±")

    # ì›ë³¸ Time ì»¬ëŸ¼ ì œê±°
    df = df.drop(columns=[time_column])
    print(f"  âœ“ ì›ë³¸ '{time_column}' ì»¬ëŸ¼ ì œê±°")

    return df, new_features


def scale_features(df, target_column=None, strategy='robust', exclude_cols=None):
    """ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ìŠ¤ì¼€ì¼ë§"""
    print(f"\nìŠ¤ì¼€ì¼ë§ ì ìš© ì¤‘ (ì „ëžµ: {strategy})...")

    # ìŠ¤ì¼€ì¼ëŸ¬ ì„ íƒ
    if strategy == 'robust':
        scaler = RobustScaler()
        print("  âœ“ RobustScaler ì„ íƒ (ì´ìƒì¹˜ì— ê°•ê±´)")
    elif strategy == 'standard':
        scaler = StandardScaler()
        print("  âœ“ StandardScaler ì„ íƒ (í‰ê·  0, ë¶„ì‚° 1)")
    elif strategy == 'minmax':
        scaler = MinMaxScaler()
        print("  âœ“ MinMaxScaler ì„ íƒ (0-1 ë²”ìœ„)")
    else:
        raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ì „ëžµ: {strategy}")

    # ìŠ¤ì¼€ì¼ë§ ëŒ€ìƒ ì»¬ëŸ¼ ì„ íƒ
    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()

    # ì œì™¸ ì»¬ëŸ¼
    if target_column and target_column in numeric_cols:
        numeric_cols.remove(target_column)

    if exclude_cols:
        numeric_cols = [col for col in numeric_cols if col not in exclude_cols]

    if not numeric_cols:
        print("  âš ï¸  ìŠ¤ì¼€ì¼ë§í•  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return df, None

    print(f"  ëŒ€ìƒ ì»¬ëŸ¼: {len(numeric_cols)}ê°œ")

    # ìŠ¤ì¼€ì¼ë§ ì ìš©
    scaled_data = scaler.fit_transform(df[numeric_cols])

    # ìŠ¤ì¼€ì¼ë§ëœ ì»¬ëŸ¼ ì´ë¦„
    scaled_col_names = [f"{col}_scaled" if col in ['Amount', 'amount'] else col
                        for col in numeric_cols]

    # DataFrame ìƒì„±
    df_scaled = pd.DataFrame(scaled_data, columns=scaled_col_names, index=df.index)

    # ì›ë³¸ ì»¬ëŸ¼ ì œê±° (Amountë§Œ)
    if 'Amount' in numeric_cols or 'amount' in numeric_cols:
        for col in ['Amount', 'amount']:
            if col in df.columns:
                df = df.drop(columns=[col])
                print(f"  âœ“ ì›ë³¸ '{col}' ì»¬ëŸ¼ ì œê±°")

    # ìŠ¤ì¼€ì¼ë§ëœ ì»¬ëŸ¼ ì¶”ê°€
    for col in df_scaled.columns:
        df[col] = df_scaled[col]

    print(f"  âœ“ ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ: {len(numeric_cols)}ê°œ ë³€ìˆ˜")

    return df, scaler


def generate_log(
    dataset_name,
    original_shape,
    final_shape,
    scaling_info,
    time_features_info,
    output_path
):
    """ë³€í™˜ ë¡œê·¸ ìƒì„±"""
    log = f"""# íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ë¡œê·¸

**ìƒì„±ì¼**: {datetime.now().strftime('%Y-%m-%d %H:%M')}
**ì›ë³¸ ë°ì´í„°**: {dataset_name} ({original_shape[0]:,}ê±´, {original_shape[1]}ê°œ íŠ¹ì„±)

---

## ì ìš©ëœ ë³€í™˜

"""

    # ìŠ¤ì¼€ì¼ë§ ì •ë³´
    if scaling_info:
        log += f"""### 1. ìŠ¤ì¼€ì¼ë§ ({scaling_info['strategy']})
- **ì „ëžµ**: {scaling_info['strategy']}
- **ëŒ€ìƒ ë³€ìˆ˜**: {', '.join(scaling_info['scaled_columns'])}
- **ë³€í™˜ í›„ ì»¬ëŸ¼**: {', '.join([f"{col}_scaled" if col in ['Amount', 'amount'] else col for col in scaling_info['scaled_columns']])}

"""

    # ì‹œê°„ íŠ¹ì„± ì •ë³´
    if time_features_info:
        log += f"""### 2. ì‹œê°„ íŠ¹ì„± ì¶”ì¶œ
- **ì›ë³¸**: {time_features_info['original_column']}
- **ìƒì„±ëœ íŠ¹ì„±**:
"""
        for feature in time_features_info['new_features']:
            log += f"  - {feature}\n"
        log += f"- **ì›ë³¸ ì»¬ëŸ¼ ì œê±°**: âœ“\n\n"

    # ë³€ìˆ˜ ìš”ì•½
    removed = original_shape[1] - final_shape[1] + len(time_features_info.get('new_features', []))
    added = len(time_features_info.get('new_features', []))

    log += f"""### 3. ë³€ìˆ˜ ìš”ì•½
- **ì›ë³¸ íŠ¹ì„±**: {original_shape[1]}ê°œ
- **ìµœì¢… íŠ¹ì„±**: {final_shape[1]}ê°œ ({final_shape[1] - original_shape[1]:+d}ê°œ)
- **ì œê±°ëœ íŠ¹ì„±**: {removed}ê°œ
- **ì¶”ê°€ëœ íŠ¹ì„±**: {added}ê°œ

---

## ë‹¤ìŒ ë‹¨ê³„
- `/handle-imbalance`: í´ëž˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬ (SMOTE)
- `/train-models`: ëª¨ë¸ í•™ìŠµ

---

**ìƒì„± ë„êµ¬**: feature-engineering plugin v1.0.0
"""

    # ë¡œê·¸ ì €ìž¥
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(log)

    return log


def main():
    parser = argparse.ArgumentParser(
        description='íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ë° ë°ì´í„° ì „ì²˜ë¦¬',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    parser.add_argument(
        '--data-path',
        type=str,
        required=True,
        help='ì›ë³¸ ë°ì´í„° íŒŒì¼ ê²½ë¡œ'
    )
    parser.add_argument(
        '--target-column',
        type=str,
        help='íƒ€ê²Ÿ ë³€ìˆ˜ ì»¬ëŸ¼ëª… (ì „ì²˜ë¦¬ì—ì„œ ì œì™¸)'
    )
    parser.add_argument(
        '--scaling-strategy',
        type=str,
        choices=['robust', 'standard', 'minmax'],
        default='robust',
        help='ìŠ¤ì¼€ì¼ë§ ì „ëžµ (ê¸°ë³¸ê°’: robust)'
    )
    parser.add_argument(
        '--time-features',
        type=str,
        help='ì‹œê°„ íŠ¹ì„± (comma-separated: hour,day,cyclical)'
    )
    parser.add_argument(
        '--output-dir',
        type=str,
        default='data/processed',
        help='ì „ì²˜ë¦¬ ë°ì´í„° ì €ìž¥ ë””ë ‰í† ë¦¬'
    )

    args = parser.parse_args()

    print("=" * 60)
    print("íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì‹œìž‘")
    print("=" * 60)

    # ë°ì´í„° ë¡œë“œ
    df = load_data(args.data_path)
    original_shape = df.shape
    dataset_name = Path(args.data_path).stem

    # íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ë¦¬
    if args.target_column and args.target_column in df.columns:
        y = df[args.target_column]
        X = df.drop(columns=[args.target_column])
        print(f"\níƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ë¦¬: {args.target_column}")
    else:
        X = df
        y = None

    # ì‹œê°„ íŠ¹ì„± ì¶”ì¶œ
    time_features_info = None
    if args.time_features:
        features = [f.strip() for f in args.time_features.split(',')]
        X, new_time_features = extract_time_features(X, features=features)
        if new_time_features:
            time_features_info = {
                'original_column': 'Time',
                'new_features': new_time_features
            }

    # ìŠ¤ì¼€ì¼ë§
    # V1-V28ì€ ì´ë¯¸ ì •ê·œí™”ë˜ì–´ ìžˆìœ¼ë¯€ë¡œ ì œì™¸
    exclude_cols = [f'V{i}' for i in range(1, 29)]  # V1-V28
    X, scaler = scale_features(
        X,
        target_column=None,
        strategy=args.scaling_strategy,
        exclude_cols=exclude_cols
    )

    scaling_info = None
    if scaler is not None:
        scaled_columns = [col for col in X.columns if 'scaled' in col or col == 'Amount']
        scaling_info = {
            'strategy': args.scaling_strategy,
            'scaled_columns': [col.replace('_scaled', '') for col in scaled_columns if '_scaled' in col]
        }

    final_shape = X.shape

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    model_dir = Path('outputs/models')
    model_dir.mkdir(parents=True, exist_ok=True)

    report_dir = Path('outputs/reports')
    report_dir.mkdir(parents=True, exist_ok=True)

    # ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ìž¥
    X_path = output_dir / f"{dataset_name}_processed_X.csv"
    X.to_csv(X_path, index=False)
    print(f"\nâœ“ íŠ¹ì„± ë°ì´í„° ì €ìž¥: {X_path}")

    if y is not None:
        y_path = output_dir / f"{dataset_name}_processed_y.csv"
        y.to_csv(y_path, index=False, header=True)
        print(f"âœ“ íƒ€ê²Ÿ ë°ì´í„° ì €ìž¥: {y_path}")

    # ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì €ìž¥
    if scaler is not None:
        pipeline_path = model_dir / f"{dataset_name}_preprocessing_pipeline.pkl"
        joblib.dump(scaler, pipeline_path)
        print(f"âœ“ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì €ìž¥: {pipeline_path}")

    # ë¡œê·¸ ìƒì„±
    log_path = report_dir / f"{dataset_name}_feature_engineering_log.md"
    generate_log(
        dataset_name,
        original_shape,
        final_shape,
        scaling_info,
        time_features_info,
        log_path
    )
    print(f"âœ“ ë³€í™˜ ë¡œê·¸ ì €ìž¥: {log_path}")

    # ìš”ì•½ ì¶œë ¥
    print(f"\n{'=' * 60}")
    print("íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ")
    print(f"{'=' * 60}")
    print(f"\nðŸ“Š ë°ì´í„°ì…‹: {dataset_name}")
    print(f"   ì›ë³¸: {original_shape[0]:,}ê±´ Ã— {original_shape[1]}ê°œ íŠ¹ì„±")
    print(f"   ìµœì¢…: {final_shape[0]:,}ê±´ Ã— {final_shape[1]}ê°œ íŠ¹ì„±")
    print(f"   ë³€í™”: {final_shape[1] - original_shape[1]:+d}ê°œ íŠ¹ì„±")

    print(f"\nðŸ“ ì¶œë ¥:")
    print(f"   íŠ¹ì„± ë°ì´í„°: {X_path}")
    if y is not None:
        print(f"   íƒ€ê²Ÿ ë°ì´í„°: {y_path}")
    if scaler is not None:
        print(f"   íŒŒì´í”„ë¼ì¸: {pipeline_path}")
    print(f"   ë¡œê·¸: {log_path}")

    print(f"\në‹¤ìŒ ë‹¨ê³„:")
    if y is not None and len(pd.Series(y).value_counts()) == 2:
        # ì´ì§„ ë¶„ë¥˜ì¸ ê²½ìš°
        value_counts = pd.Series(y).value_counts()
        imbalance_ratio = value_counts.max() / value_counts.min()
        if imbalance_ratio > 10:
            print("   /handle-imbalance --method smote")
    print("   /train-models --algorithms xgboost,lightgbm\n")


if __name__ == "__main__":
    main()
