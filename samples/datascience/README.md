# ì‹ ìš©ì¹´ë“œ ì‚¬ê¸° íƒì§€ (Credit Card Fraud Detection)

## ğŸ“Š ë°ì´í„°ì…‹ ê°œìš”

**ì¶œì²˜**: [Kaggle - Credit Card Fraud Detection](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)
**ë¼ì´ì„ ìŠ¤**: DbCL-1.0
**ë°ì´í„° ê¸°ê°„**: 2013ë…„ 9ì›” (ìœ ëŸ½ ì¹´ë“œ ì†Œì§€ìì˜ 2ì¼ê°„ ê±°ë˜ ë°ì´í„°)

### ê¸°ë³¸ í†µê³„
- **ì „ì²´ ê±°ë˜ ê±´ìˆ˜**: 284,807ê±´
- **íŠ¹ì„± ê°œìˆ˜**: 31ê°œ (Time, V1-V28, Amount, Class)
- **ê²°ì¸¡ì¹˜**: 0ê°œ
- **í´ë˜ìŠ¤ ë¶„í¬**:
  - ì •ìƒ ê±°ë˜ (Class 0): 284,315ê±´ (99.83%)
  - ì‚¬ê¸° ê±°ë˜ (Class 1): 492ê±´ (0.17%)
  - **ë¶ˆê· í˜• ë¹„ìœ¨**: 1:578

---

## ğŸ“ ë°ì´í„° êµ¬ì¡°

### íŠ¹ì„± ì„¤ëª…

| íŠ¹ì„± | ì„¤ëª… | ë¹„ê³  |
|------|------|------|
| **Time** | ì²« ê±°ë˜ ì´í›„ ê²½ê³¼ ì‹œê°„(ì´ˆ) | ì‹œê³„ì—´ ë¶„ì„ ê°€ëŠ¥ |
| **V1 ~ V28** | PCA ë³€í™˜ëœ ìµëª…í™” íŠ¹ì„± | ê°œì¸ì •ë³´ ë³´í˜¸ë¥¼ ìœ„í•œ ë³€í™˜ |
| **Amount** | ê±°ë˜ ê¸ˆì•¡ | ì›ë³¸ ê°’ (ìŠ¤ì¼€ì¼ë§ í•„ìš”) |
| **Class** | íƒ€ê²Ÿ ë³€ìˆ˜ | 0: ì •ìƒ, 1: ì‚¬ê¸° |

âš ï¸ **ì£¼ì˜**: V1~V28ì€ PCAë¡œ ë³€í™˜ë˜ì–´ ì§ì ‘ì ì¸ ë¹„ì¦ˆë‹ˆìŠ¤ í•´ì„ì´ ì–´ë µìŠµë‹ˆë‹¤. ë³´ì•ˆìƒì˜ ì´ìœ ë¡œ ì›ë³¸ íŠ¹ì„±ëª…ê³¼ ë°°ê²½ ì •ë³´ê°€ ì œê³µë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

---

## ğŸ¯ í”„ë¡œì íŠ¸ ëª©í‘œ

ì´ ë°ì´í„°ì…‹ì€ **ê³ ë„ë¡œ ë¶ˆê· í˜•í•œ ì´ì§„ ë¶„ë¥˜ ë¬¸ì œ**ë¥¼ ë‹¤ë£¨ëŠ” ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•ì„ ìœ„í•œ ìƒ˜í”Œì…ë‹ˆë‹¤.

### ì£¼ìš” í•™ìŠµ í¬ì¸íŠ¸

#### 1. **ë°ì´í„° í”„ë¡œíŒŒì¼ë§ (Data Profiling)**
- [ ] íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ (EDA)
- [ ] í´ë˜ìŠ¤ ë¶ˆê· í˜• ì‹œê°í™”
- [ ] íŠ¹ì„± ë¶„í¬ ë¶„ì„
- [ ] ìƒê´€ê´€ê³„ ë¶„ì„
- [ ] ì´ìƒì¹˜ íƒì§€

#### 2. **íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ (Feature Engineering)**
- [ ] Time íŠ¹ì„± ë³€í™˜ (ì‹œê°„ëŒ€, ìš”ì¼ ë“±)
- [ ] Amount ìŠ¤ì¼€ì¼ë§ (StandardScaler, RobustScaler)
- [ ] íŠ¹ì„± ì„ íƒ (Correlation, Feature Importance)
- [ ] íŒŒìƒ ë³€ìˆ˜ ìƒì„±

#### 3. **ë¶ˆê· í˜• ì²˜ë¦¬ (Imbalance Handling)**
- [ ] **ë¦¬ìƒ˜í”Œë§ ê¸°ë²•**:
  - SMOTE (Synthetic Minority Over-sampling Technique)
  - Random Undersampling
  - ADASYN
  - SMOTETomek (Combined)
- [ ] **ì•Œê³ ë¦¬ì¦˜ ë ˆë²¨ ì¡°ì •**:
  - class_weight='balanced'
  - scale_pos_weight (XGBoost)
  - sample_weight
- [ ] **ì•™ìƒë¸” ê¸°ë²•**:
  - BalancedRandomForest
  - EasyEnsemble
  - BalancedBagging

#### 4. **ëª¨ë¸ë§ (Modeling)**
- [ ] **ì „í†µì  ML ì•Œê³ ë¦¬ì¦˜**:
  - Logistic Regression (ë² ì´ìŠ¤ë¼ì¸)
  - Random Forest
  - XGBoost
  - LightGBM
  - CatBoost
- [ ] **ì´ìƒíƒì§€ ì•Œê³ ë¦¬ì¦˜**:
  - Isolation Forest
  - Local Outlier Factor (LOF)
  - One-Class SVM
  - Autoencoder
- [ ] í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (Optuna, GridSearchCV)

#### 5. **í‰ê°€ ë° í•´ì„ (Evaluation & Interpretation)**
- [ ] **ì í•©í•œ í‰ê°€ì§€í‘œ ì‚¬ìš©**:
  - âš ï¸ AccuracyëŠ” ë¶€ì í•© (99.83% ë¶ˆê· í˜•)
  - âœ… Precision, Recall, F1-Score
  - âœ… PR-AUC (Precision-Recall AUC)
  - âœ… ROC-AUC
  - âœ… Confusion Matrix
  - âœ… Cost-Sensitive Metrics
- [ ] **ì„ê³„ê°’ ìµœì í™”**:
  - Precision-Recall íŠ¸ë ˆì´ë“œì˜¤í”„
  - ë¹„ì¦ˆë‹ˆìŠ¤ ë¹„ìš© ê³ ë ¤ (FP vs FN)
- [ ] **ëª¨ë¸ í•´ì„**:
  - SHAP Values
  - Feature Importance
  - LIME
  - Partial Dependence Plots

#### 6. **ë³´ê³ ì„œ ì‘ì„± (Reporting)**
- [ ] ë¹„ê¸°ìˆ  ì´í•´ê´€ê³„ììš© ìš”ì•½
- [ ] ëª¨ë¸ ì„±ëŠ¥ ë¹„êµí‘œ
- [ ] ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ ê³„ì‚°
- [ ] ë°°í¬ ê¶Œì¥ì‚¬í•­

---

## ğŸš€ ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ íŒŒì´í”„ë¼ì¸ êµ¬ì¡°

```
Raw Data (creditcard.csv)
    â†“
data-profiling â†’ profile_report.html
    â†“
feature-engineering â†’ preprocessing_pipeline.pkl
    â†“
imbalance-handling â†’ X_train_balanced.csv
    â†“
model-selection â†’ trained_models/
    â†“
evaluation-report â†’ evaluation_report.pdf
    â†“
deployment-package â†’ model_api/
```

---

## ğŸ’¡ ì£¼ìš” ë„ì „ ê³¼ì œ

### 1. ê·¹ì‹¬í•œ í´ë˜ìŠ¤ ë¶ˆê· í˜• (1:578)
- Accuracy ì§€í‘œëŠ” ë¬´ì˜ë¯¸ (ëª¨ë“  ê±°ë˜ë¥¼ ì •ìƒìœ¼ë¡œ ì˜ˆì¸¡í•´ë„ 99.83% ë‹¬ì„±)
- Precision-Recall ê³¡ì„ ì´ ROC ê³¡ì„ ë³´ë‹¤ ì¤‘ìš”
- ì†Œìˆ˜ í´ë˜ìŠ¤(ì‚¬ê¸°)ì— ëŒ€í•œ ì¶©ë¶„í•œ í•™ìŠµ í•„ìš”

### 2. ë¹„ìš© ë¯¼ê° ë¶„ë¥˜ (Cost-Sensitive Learning)
- **False Negative (ì‚¬ê¸°ë¥¼ ì •ìƒìœ¼ë¡œ ì˜¤íŒ)**: ê¸ˆì „ì  ì†ì‹¤
- **False Positive (ì •ìƒì„ ì‚¬ê¸°ë¡œ ì˜¤íŒ)**: ê³ ê° ë¶ˆí¸, ì‹ ë¢° í•˜ë½
- ë¹„ì¦ˆë‹ˆìŠ¤ ì»¨í…ìŠ¤íŠ¸ì— ë”°ë¼ FN/FP ë¹„ìš© ë¹„ìœ¨ ê²°ì • í•„ìš”

### 3. PCA ë³€í™˜ëœ íŠ¹ì„±
- ì›ë³¸ íŠ¹ì„±ëª… ë¶ˆëª… â†’ ë„ë©”ì¸ ì§€ì‹ í™œìš© ì œí•œ
- Feature Engineering ì–´ë ¤ì›€
- ëª¨ë¸ í•´ì„ì„± ì €í•˜

### 4. ì‹œê°„ì  ì˜ì¡´ì„±
- Time ë³€ìˆ˜ ì¡´ì¬ â†’ ì‹œê³„ì—´ íŠ¹ì„± ê³ ë ¤
- Train/Test Split ì‹œ ì‹œê°„ ìˆœì„œ ë³´ì¡´ í•„ìš”
- ì‹œê°„ëŒ€ë³„ ì‚¬ê¸° íŒ¨í„´ ì¡´ì¬ ê°€ëŠ¥ì„±

---

## ğŸ“ˆ ì˜ˆìƒ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

| ëª¨ë¸ | Precision | Recall | F1-Score | PR-AUC |
|------|-----------|--------|----------|--------|
| Baseline (ëª¨ë‘ ì •ìƒ ì˜ˆì¸¡) | - | 0% | - | - |
| Logistic Regression | ~0.85 | ~0.60 | ~0.70 | ~0.75 |
| Random Forest | ~0.90 | ~0.75 | ~0.82 | ~0.85 |
| XGBoost + SMOTE | ~0.92 | ~0.80 | ~0.86 | ~0.90 |
| Isolation Forest | ~0.25 | ~0.30 | ~0.27 | ~0.60 |

*ì‹¤ì œ ì„±ëŠ¥ì€ ì „ì²˜ë¦¬, í•˜ì´í¼íŒŒë¼ë¯¸í„°, ë¶ˆê· í˜• ì²˜ë¦¬ ì „ëµì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.*

---

## ğŸ”§ ì‚¬ìš© ì˜ˆì‹œ

### ë°ì´í„° ë¡œë“œ
```python
import pandas as pd
import numpy as np

# ë°ì´í„° ë¡œë“œ
df = pd.read_csv('creditcard.csv')

# íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
X = df.drop('Class', axis=1)
y = df['Class']

# Train/Test ë¶„ë¦¬ (ì‹œê°„ ìˆœì„œ ë³´ì¡´)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
```

### SMOTE ì ìš©
```python
from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)
```

### ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score

# ëª¨ë¸ í•™ìŠµ
model = RandomForestClassifier(
    n_estimators=100,
    class_weight='balanced',
    random_state=42
)
model.fit(X_train_balanced, y_train_balanced)

# ì˜ˆì¸¡ ë° í‰ê°€
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]

print(classification_report(y_test, y_pred))
print(f"ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}")
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [Kaggle ë°ì´í„°ì…‹ í˜ì´ì§€](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)
- [imbalanced-learn ê³µì‹ ë¬¸ì„œ](https://imbalanced-learn.org/)
- [SMOTE ë…¼ë¬¸](https://arxiv.org/abs/1106.1813)
- [Cost-Sensitive Learning](https://machinelearningmastery.com/cost-sensitive-learning-for-imbalanced-classification/)

---

## ğŸ“ ë¼ì´ì„ ìŠ¤

ë°ì´í„°ì…‹ì€ Database Contents License (DbCL v1.0) í•˜ì— ë°°í¬ë©ë‹ˆë‹¤.

**ì¸ìš©**:
```
Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi.
Calibrating Probability with Undersampling for Unbalanced Classification.
In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015
```

---

**ìƒì„±ì¼**: 2026-01-31
**í”„ë¡œì íŠ¸**: DanteLabs Agentic School - Data Science Plugin
