# Model Selection Plugin

ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ëŠ” í”ŒëŸ¬ê·¸ì¸ì…ë‹ˆë‹¤.

## ğŸ“‹ ê°œìš”

ì´ í”ŒëŸ¬ê·¸ì¸ì€ ì „ì²˜ë¦¬ ë° ë¦¬ìƒ˜í”Œë§ ì™„ë£Œëœ ë°ì´í„°ë¡œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  í‰ê°€í•©ë‹ˆë‹¤:

- âœ… **ì•Œê³ ë¦¬ì¦˜**: XGBoost, LightGBM, Random Forest
- âœ… **í‰ê°€ ì§€í‘œ**: ROC-AUC, PR-AUC, F1-Score, Confusion Matrix
- âœ… **ëª¨ë¸ ì €ì¥**: Joblibë¡œ ì¬ì‚¬ìš© ê°€ëŠ¥
- âœ… **Feature Importance**: ì¤‘ìš” ë³€ìˆ˜ ë¶„ì„ (ì˜ˆì •)
- âœ… **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**: Optuna í†µí•© (ì˜ˆì •)

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. ì˜ì¡´ì„± ì„¤ì¹˜

```bash
cd plugins/model-selection/skills/model-selection
uv pip install --system -r requirements.txt
```

### 2. ëª¨ë¸ í•™ìŠµ

```bash
# XGBoost í•™ìŠµ (ê¸°ë³¸ê°’, ê¶Œì¥)
python scripts/train_model.py \
  --X-train-path "projects/creditcard-fraud-detection/data/processed/X_train_balanced.csv" \
  --y-train-path "projects/creditcard-fraud-detection/data/processed/y_train_balanced.csv" \
  --X-test-path "projects/creditcard-fraud-detection/data/processed/X_test.csv" \
  --y-test-path "projects/creditcard-fraud-detection/data/processed/y_test.csv" \
  --algorithm xgboost
```

## ğŸ“ í”ŒëŸ¬ê·¸ì¸ êµ¬ì¡°

```
plugins/model-selection/
â”œâ”€â”€ plugin.json
â”œâ”€â”€ README.md
â”œâ”€â”€ agents/
â”‚   â””â”€â”€ model-trainer.md
â”œâ”€â”€ commands/
â”‚   â””â”€â”€ train-model.md
â””â”€â”€ skills/
    â””â”€â”€ model-selection/
        â”œâ”€â”€ requirements.txt
        â””â”€â”€ scripts/
            â””â”€â”€ train_model.py
```

## ğŸ¯ ì§€ì› ì•Œê³ ë¦¬ì¦˜

### 1. XGBoost (ê¸°ë³¸ê°’, ê¶Œì¥)
```python
import xgboost as xgb

model = xgb.XGBClassifier(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    random_state=42
)
```

**ì¥ì **:
- âœ… ë†’ì€ ì„±ëŠ¥
- âœ… ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬ ê°•ì 
- âœ… Feature importance ì œê³µ
- âœ… ì •ê·œí™” ë‚´ì¥

**ì‚¬ìš© ì‹œê¸°**: ëŒ€ë¶€ë¶„ì˜ ê²½ìš° (ê¸°ë³¸ ì¶”ì²œ)

### 2. LightGBM
```python
import lightgbm as lgb

model = lgb.LGBMClassifier(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    random_state=42
)
```

**ì¥ì **:
- âœ… XGBoostë³´ë‹¤ ë¹ ë¦„
- âœ… ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
- âœ… ë²”ì£¼í˜• ë³€ìˆ˜ ì§ì ‘ ì²˜ë¦¬

**ì‚¬ìš© ì‹œê¸°**: ëŒ€ìš©ëŸ‰ ë°ì´í„° (100ë§Œ ê±´+)

### 3. Random Forest
```python
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    random_state=42
)
```

**ì¥ì **:
- âœ… í•´ì„ ê°€ëŠ¥
- âœ… ì•ˆì •ì 
- âœ… ê³¼ì í•© ëœí•¨

**ì‚¬ìš© ì‹œê¸°**: ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸, í•´ì„ ì¤‘ìš” ì‹œ

## ğŸ“Š ì¶œë ¥

### í•™ìŠµëœ ëª¨ë¸
```
projects/{project-name}/outputs/models/
â”œâ”€â”€ xgboost_model.pkl           # í•™ìŠµëœ ëª¨ë¸
â””â”€â”€ preprocessing_pipeline.pkl  # ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
```

### ì½˜ì†” ì¶œë ¥
```
============================================================
ëª¨ë¸ í•™ìŠµ ì‹œì‘
============================================================

ë°ì´í„° ë¡œë“œ ì¤‘...
âœ“ Train: 250,196ê±´
âœ“ Test: 56,962ê±´

ëª¨ë¸ í•™ìŠµ ì¤‘ (ì•Œê³ ë¦¬ì¦˜: xgboost)...
âœ“ í•™ìŠµ ì™„ë£Œ

ëª¨ë¸ í‰ê°€ ì¤‘...

============================================================
ë¶„ë¥˜ ë¦¬í¬íŠ¸
============================================================
              precision    recall  f1-score   support

           0       1.00      1.00      1.00     56864
           1       0.81      0.85      0.83        98

    accuracy                           1.00     56962
   macro avg       0.90      0.92      0.91     56962
weighted avg       1.00      1.00      1.00     56962

ROC-AUC: 0.9760
PR-AUC: 0.8701

Confusion Matrix:
                Predicted
              0        1
Actual 0  56,844      20
Actual 1      15      83

âœ“ ëª¨ë¸ ì €ì¥: projects/creditcard-fraud-detection/outputs/models/xgboost_model.pkl

============================================================
ëª¨ë¸ í•™ìŠµ ì™„ë£Œ
============================================================

ğŸ“Š ìµœì¢… ì„±ëŠ¥:
   ROC-AUC: 0.9760
   PR-AUC: 0.8701
```

## ğŸ”§ ì‚¬ìš© ì˜ˆì‹œ

### Example 1: ì‹ ìš©ì¹´ë“œ ì‚¬ê¸° íƒì§€ (XGBoost)
```bash
python train_model.py \
  --X-train-path "projects/creditcard-fraud-detection/data/processed/X_train_balanced.csv" \
  --y-train-path "projects/creditcard-fraud-detection/data/processed/y_train_balanced.csv" \
  --X-test-path "projects/creditcard-fraud-detection/data/processed/X_test.csv" \
  --y-test-path "projects/creditcard-fraud-detection/data/processed/y_test.csv" \
  --algorithm xgboost
```

**ì˜ˆìƒ ì„±ëŠ¥**:
- ROC-AUC: 0.97+
- PR-AUC: 0.87+
- F1-Score: 0.83+

### Example 2: ê³ ê° ì´íƒˆ ì˜ˆì¸¡ (LightGBM)
```bash
python train_model.py \
  --X-train-path "projects/customer-churn/data/processed/X_train_balanced.csv" \
  --y-train-path "projects/customer-churn/data/processed/y_train_balanced.csv" \
  --X-test-path "projects/customer-churn/data/processed/X_test.csv" \
  --y-test-path "projects/customer-churn/data/processed/y_test.csv" \
  --algorithm lightgbm
```

### Example 3: ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ (Random Forest)
```bash
python train_model.py \
  --X-train-path "projects/my-project/data/processed/X_train_balanced.csv" \
  --y-train-path "projects/my-project/data/processed/y_train_balanced.csv" \
  --X-test-path "projects/my-project/data/processed/X_test.csv" \
  --y-test-path "projects/my-project/data/processed/y_test.csv" \
  --algorithm random_forest
```

## ğŸ“ˆ í‰ê°€ ì§€í‘œ ê°€ì´ë“œ

### ë¶ˆê· í˜• ë°ì´í„° (ì‚¬ê¸° íƒì§€, ì´ìƒ íƒì§€)

| ì§€í‘œ | ì‚¬ìš© ì—¬ë¶€ | ì´ìœ  |
|------|----------|------|
| **Accuracy** | âŒ ê¸ˆì§€ | ë¶ˆê· í˜•ì—ì„œ ë¬´ì˜ë¯¸ |
| **Precision** | âœ… ì¤‘ìš” | FP ë¹„ìš© ê³ ë ¤ |
| **Recall** | âœ… ë§¤ìš° ì¤‘ìš” | FN ë¹„ìš© ê³ ë ¤ |
| **F1-Score** | âœ… í•µì‹¬ | Precision-Recall ê· í˜• |
| **PR-AUC** | âœ… ìµœì  | ë¶ˆê· í˜• ë°ì´í„° ìµœì  ì§€í‘œ |
| **ROC-AUC** | âš ï¸ ì°¸ê³  | PR-AUCë³´ë‹¤ ëœ ìœ ìš© |

### Confusion Matrix í•´ì„

```
                Predicted
              0        1
Actual 0  56,844      20    # TN, FP
Actual 1      15      83    # FN, TP
```

- **TP (True Positive)**: ì‚¬ê¸°ë¥¼ ì‚¬ê¸°ë¡œ ì •í™•íˆ ì˜ˆì¸¡
- **TN (True Negative)**: ì •ìƒì„ ì •ìƒìœ¼ë¡œ ì •í™•íˆ ì˜ˆì¸¡
- **FP (False Positive)**: ì •ìƒì„ ì‚¬ê¸°ë¡œ ì˜¤íŒ (Type I Error)
- **FN (False Negative)**: ì‚¬ê¸°ë¥¼ ì •ìƒìœ¼ë¡œ ì˜¤íŒ (Type II Error) âš ï¸

**ë¹„ì¦ˆë‹ˆìŠ¤ ì˜í–¥**:
- **FN (ì‚¬ê¸° ë†“ì¹¨)**: ê¸ˆì „ì  ì†ì‹¤ â†’ Recall ì¤‘ìš”
- **FP (ì •ìƒ ì˜¤íŒ)**: ê³ ê° ë¶ˆí¸ â†’ Precision ê³ ë ¤

## ğŸ” ëª¨ë¸ í™œìš©

### í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ & ì˜ˆì¸¡
```python
import joblib
import pandas as pd

# ëª¨ë¸ ë¡œë“œ
model = joblib.load('projects/creditcard-fraud-detection/outputs/models/xgboost_model.pkl')
scaler = joblib.load('projects/creditcard-fraud-detection/outputs/models/preprocessing_pipeline.pkl')

# ì‹ ê·œ ë°ì´í„° ì „ì²˜ë¦¬
X_new = pd.read_csv('new_data.csv')
X_new_scaled = scaler.transform(X_new)

# ì˜ˆì¸¡
y_pred = model.predict(X_new_scaled)
y_proba = model.predict_proba(X_new_scaled)[:, 1]

print(f"ì‚¬ê¸° í™•ë¥ : {y_proba[0]:.2%}")
```

### Threshold ìµœì í™”
```python
from sklearn.metrics import precision_recall_curve
import numpy as np

# ìµœì  ì„ê³„ê°’ ì°¾ê¸°
precision, recall, thresholds = precision_recall_curve(y_test, y_proba)
f1_scores = 2 * (precision * recall) / (precision + recall)
optimal_threshold = thresholds[np.argmax(f1_scores)]

print(f"ìµœì  ì„ê³„ê°’: {optimal_threshold:.3f}")

# ì ìš©
y_pred_optimized = (y_proba >= optimal_threshold).astype(int)
```

## ğŸ› íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ: ê³¼ì í•© (Train >> Test ì„±ëŠ¥)
**í•´ê²°**:
```python
# max_depth ì¤„ì´ê¸°
model = xgb.XGBClassifier(max_depth=3)

# ì •ê·œí™” ê°•í™”
model = xgb.XGBClassifier(reg_alpha=1.0, reg_lambda=10.0)

# ì•™ìƒë¸” í¬ê¸° ì¤„ì´ê¸°
model = xgb.XGBClassifier(n_estimators=50)
```

### ë¬¸ì œ: ì €ì„±ëŠ¥ (F1-Score < 0.5)
**í•´ê²°**:
- ë¦¬ìƒ˜í”Œë§ ë¹„ìœ¨ ì¡°ì • (`/balance-data --ratio 0.2`)
- ì „ì²˜ë¦¬ ì¬í™•ì¸ (`/engineer-features`)
- ì•Œê³ ë¦¬ì¦˜ ë³€ê²½ (RF â†’ XGBoost)
- í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

### ë¬¸ì œ: ë©”ëª¨ë¦¬ ë¶€ì¡±
**í•´ê²°**:
- LightGBM ì‚¬ìš©
- n_estimators ì¤„ì´ê¸°
- ì²­í¬ ë‹¨ìœ„ í•™ìŠµ

## ğŸ“Š ì•Œê³ ë¦¬ì¦˜ ë¹„êµ

| ì•Œê³ ë¦¬ì¦˜ | ì†ë„ | ì„±ëŠ¥ | ë©”ëª¨ë¦¬ | í•´ì„ì„± | ì¶”ì²œ ìˆœìœ„ |
|---------|------|------|--------|--------|----------|
| **XGBoost** | ë³´í†µ | ë§¤ìš° ìš°ìˆ˜ | ë³´í†µ | ì¤‘ê°„ | â­â­â­ |
| **LightGBM** | ë¹ ë¦„ | ë§¤ìš° ìš°ìˆ˜ | ìš°ìˆ˜ | ì¤‘ê°„ | â­â­ |
| **Random Forest** | ëŠë¦¼ | ìš°ìˆ˜ | ë‚˜ì¨ | ìš°ìˆ˜ | â­ |

## ğŸ”— ê´€ë ¨ í”ŒëŸ¬ê·¸ì¸

- `data-profiling`: ë°ì´í„° ë¶„ì„
- `feature-engineering`: íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§
- `imbalance-handling`: í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬ (í•™ìŠµ ì „)

## ğŸ“ ë¼ì´ì„ ìŠ¤

MIT License

## ğŸ‘¤ ì‘ì„±ì

- **Dante Labs**
- Email: datapod.k@gmail.com
- ë²„ì „: 1.0.0

## ğŸ’¡ Best Practices

### í•™ìŠµ ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ (`/engineer-features`)
- [ ] í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬ (`/balance-data`)
- [ ] Train/Test ë¶„ë¦¬ í™•ì¸
- [ ] íŠ¹ì„± ê°œìˆ˜ ì¼ì¹˜ í™•ì¸

### í•™ìŠµ í›„ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] Confusion Matrix ë¶„ì„
- [ ] F1-Score, PR-AUC ê¸°ë¡
- [ ] ê³¼ì í•© ì—¬ë¶€ í™•ì¸
- [ ] ëª¨ë¸ íŒŒì¼ ì €ì¥ í™•ì¸

### í”„ë¡œë•ì…˜ ë°°í¬ ì „
- [ ] ëª¨ë¸ + ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í•¨ê»˜ ì €ì¥
- [ ] Threshold ìµœì í™”
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê¸°ë¡
- [ ] ì˜ˆì¸¡ ì†ë„ ì¸¡ì •

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

ëª¨ë¸ í•™ìŠµ í›„ ê¶Œì¥ ë‹¨ê³„:

1. **Feature Importance ë¶„ì„** (ì˜ˆì •)
   - ì¤‘ìš” ë³€ìˆ˜ Top 20 íŒŒì•…
   - ë¶ˆí•„ìš”í•œ ë³€ìˆ˜ ì œê±°

2. **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹** (ì˜ˆì •)
   - Optuna, GridSearch
   - F1-Score ìµœì í™”

3. **Ensemble** (ì˜ˆì •)
   - XGBoost + LightGBM + RF
   - Voting, Stacking

4. **SHAP ë¶„ì„** (ì˜ˆì •)
   - ì˜ˆì¸¡ ì„¤ëª…
   - ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ
