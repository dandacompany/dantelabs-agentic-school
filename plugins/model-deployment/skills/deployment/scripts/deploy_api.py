#!/usr/bin/env python3
"""
ëª¨ë¸ ë°°í¬ API ìƒì„± ìŠ¤í¬ë¦½íŠ¸

FastAPIë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ REST APIë¡œ ë°°í¬í•©ë‹ˆë‹¤.

ì„¤ì¹˜:
    cd plugins/model-deployment/skills/deployment
    uv pip install -r requirements.txt

ì‚¬ìš©ë²•:
    python deploy_api.py --model-path "./models/model.pkl" --feature-names "V1,V2,V3,Amount"
    python deploy_api.py --model-path "./models/model.pkl" --sample-data "./data/train.csv" --target-column "Class"

ì‹¤í–‰:
    uvicorn app:app --reload --host 0.0.0.0 --port 8000

í•„ìš” íŒ¨í‚¤ì§€:
    - fastapi
    - uvicorn
    - pydantic
    - joblib
"""

import argparse
import os
import sys
from pathlib import Path
from typing import List

import joblib
import pandas as pd


def print_header(text):
    """í—¤ë” ì¶œë ¥"""
    print(f"\n{'=' * 60}")
    print(text)
    print('=' * 60)


def print_section(text):
    """ì„¹ì…˜ ì¶œë ¥"""
    print(f"\n{'-' * 60}")
    print(text)
    print('-' * 60)


def load_model(model_path):
    """ëª¨ë¸ ë¡œë“œ"""
    print(f"\nâœ“ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_path}")

    if not os.path.exists(model_path):
        raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")

    model = joblib.load(model_path)
    print(f"âœ“ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {type(model).__name__}")

    return model


def get_feature_names(sample_data_path, target_column):
    """ìƒ˜í”Œ ë°ì´í„°ì—ì„œ íŠ¹ì„± ì´ë¦„ ì¶”ì¶œ"""
    print(f"\nâœ“ íŠ¹ì„± ì´ë¦„ ì¶”ì¶œ ì¤‘: {sample_data_path}")

    df = pd.read_csv(sample_data_path)

    if target_column and target_column in df.columns:
        features = [col for col in df.columns if col != target_column]
    else:
        features = df.columns.tolist()

    print(f"âœ“ íŠ¹ì„± ê°œìˆ˜: {len(features)}ê°œ")

    return features


def generate_api_code(model_path, feature_names, output_dir, task_type='classification'):
    """FastAPI ì½”ë“œ ìƒì„±"""
    print_section("FastAPI ì½”ë“œ ìƒì„±")

    model_name = Path(model_path).stem

    # Pydantic ëª¨ë¸ ì •ì˜ (ì…ë ¥ ê²€ì¦)
    features_str = ',\n        '.join([f"{feat}: float" for feat in feature_names])

    # ì˜ˆì¸¡ íƒ€ì…
    if task_type == 'classification':
        prediction_response = """class PredictionResponse(BaseModel):
    prediction: int
    probability: Optional[List[float]] = None"""
    else:
        prediction_response = """class PredictionResponse(BaseModel):
    prediction: float"""

    api_code = f'''"""
FastAPI Model Serving
Generated by model-deployment plugin

ì‹¤í–‰:
    uvicorn app:app --reload --host 0.0.0.0 --port 8000

API ë¬¸ì„œ:
    http://localhost:8000/docs
"""

from typing import List, Optional

import joblib
import numpy as np
import pandas as pd
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

# FastAPI ì•±
app = FastAPI(
    title="{model_name} API",
    description="Machine Learning Model Serving API",
    version="1.0.0"
)

# ëª¨ë¸ ë¡œë“œ (ì„œë²„ ì‹œì‘ ì‹œ 1íšŒ)
MODEL_PATH = "{model_path}"
model = joblib.load(MODEL_PATH)

# íŠ¹ì„± ì´ë¦„
FEATURE_NAMES = {feature_names}


# Pydantic ëª¨ë¸ (ì…ë ¥ ê²€ì¦)
class PredictionRequest(BaseModel):
    {features_str}

    class Config:
        json_schema_extra = {{
            "example": {{
                {', '.join([f'"{feat}": 1.0' for feat in feature_names[:3]])}
            }}
        }}


# Pydantic ëª¨ë¸ (ì‘ë‹µ)
{prediction_response}


class HealthResponse(BaseModel):
    status: str
    model_type: str
    feature_count: int


@app.get("/", response_model=dict)
async def root():
    """API ë£¨íŠ¸"""
    return {{
        "message": "Model API is running",
        "docs": "/docs",
        "health": "/health",
        "predict": "/predict"
    }}


@app.get("/health", response_model=HealthResponse)
async def health():
    """í—¬ìŠ¤ ì²´í¬"""
    return {{
        "status": "healthy",
        "model_type": type(model).__name__,
        "feature_count": len(FEATURE_NAMES)
    }}


@app.post("/predict", response_model=PredictionResponse)
async def predict(request: PredictionRequest):
    """ì˜ˆì¸¡ ìˆ˜í–‰"""
    try:
        # ì…ë ¥ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        input_data = pd.DataFrame([request.dict()], columns=FEATURE_NAMES)

        # ì˜ˆì¸¡
        prediction = model.predict(input_data)[0]

        # í™•ë¥  (ë¶„ë¥˜ ëª¨ë¸ì¸ ê²½ìš°)
        if hasattr(model, 'predict_proba'):
            probabilities = model.predict_proba(input_data)[0].tolist()
            return {{
                "prediction": int(prediction),
                "probability": probabilities
            }}
        else:
            return {{
                "prediction": float(prediction)
            }}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/batch_predict", response_model=List[PredictionResponse])
async def batch_predict(requests: List[PredictionRequest]):
    """ë°°ì¹˜ ì˜ˆì¸¡"""
    try:
        # ì…ë ¥ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        input_data = pd.DataFrame([req.dict() for req in requests], columns=FEATURE_NAMES)

        # ì˜ˆì¸¡
        predictions = model.predict(input_data)

        # ê²°ê³¼ ìƒì„±
        results = []
        if hasattr(model, 'predict_proba'):
            probabilities = model.predict_proba(input_data)
            for pred, prob in zip(predictions, probabilities):
                results.append({{
                    "prediction": int(pred),
                    "probability": prob.tolist()
                }})
        else:
            for pred in predictions:
                results.append({{"prediction": float(pred)}})

        return results

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
'''

    # íŒŒì¼ ì €ì¥
    api_path = os.path.join(output_dir, 'app.py')
    with open(api_path, 'w', encoding='utf-8') as f:
        f.write(api_code)

    print(f"âœ“ FastAPI ì½”ë“œ ì €ì¥: {api_path}")

    return api_path


def generate_dockerfile(output_dir, requirements_path=None):
    """Dockerfile ìƒì„±"""
    print_section("Dockerfile ìƒì„±")

    dockerfile_content = '''FROM python:3.10-slim

WORKDIR /app

# ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì—…ë°ì´íŠ¸
RUN apt-get update && apt-get install -y \\
    build-essential \\
    && rm -rf /var/lib/apt/lists/*

# Python íŒ¨í‚¤ì§€ ì„¤ì¹˜
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ ë³µì‚¬
COPY app.py .
COPY model.pkl .

# í¬íŠ¸ ë…¸ì¶œ
EXPOSE 8000

# ì‹¤í–‰ ëª…ë ¹ì–´
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]
'''

    dockerfile_path = os.path.join(output_dir, 'Dockerfile')
    with open(dockerfile_path, 'w', encoding='utf-8') as f:
        f.write(dockerfile_content)

    print(f"âœ“ Dockerfile ì €ì¥: {dockerfile_path}")

    return dockerfile_path


def generate_docker_compose(output_dir):
    """docker-compose.yml ìƒì„±"""
    print_section("docker-compose.yml ìƒì„±")

    compose_content = '''version: '3.8'

services:
  model-api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - PYTHONUNBUFFERED=1
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
'''

    compose_path = os.path.join(output_dir, 'docker-compose.yml')
    with open(compose_path, 'w', encoding='utf-8') as f:
        f.write(compose_content)

    print(f"âœ“ docker-compose.yml ì €ì¥: {compose_path}")

    return compose_path


def generate_requirements_txt(output_dir):
    """requirements.txt ìƒì„±"""
    requirements_content = '''# FastAPI & Server
fastapi>=0.109.0
uvicorn[standard]>=0.27.0
pydantic>=2.5.0

# ML
scikit-learn>=1.3.0
pandas>=2.0.0
numpy>=1.24.0
joblib>=1.3.0

# Optional: Advanced models
xgboost>=2.0.0
lightgbm>=4.0.0
'''

    requirements_path = os.path.join(output_dir, 'requirements.txt')
    with open(requirements_path, 'w', encoding='utf-8') as f:
        f.write(requirements_content)

    print(f"âœ“ requirements.txt ì €ì¥: {requirements_path}")

    return requirements_path


def generate_readme(output_dir, model_name):
    """README.md ìƒì„±"""
    print_section("README.md ìƒì„±")

    readme_content = f'''# {model_name} API

FastAPI ê¸°ë°˜ Machine Learning ëª¨ë¸ ì„œë¹™ APIì…ë‹ˆë‹¤.

## ë¹ ë¥¸ ì‹œì‘

### 1. ë¡œì»¬ ì‹¤í–‰

```bash
# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# API ì„œë²„ ì‹¤í–‰
uvicorn app:app --reload --host 0.0.0.0 --port 8000
```

API ë¬¸ì„œ: http://localhost:8000/docs

### 2. Docker ì‹¤í–‰

```bash
# Docker ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t {model_name}-api .

# ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker run -p 8000:8000 {model_name}-api
```

### 3. Docker Compose ì‹¤í–‰

```bash
docker-compose up -d
```

## API ì—”ë“œí¬ì¸íŠ¸

### GET /
API ì •ë³´ í™•ì¸

### GET /health
í—¬ìŠ¤ ì²´í¬
- ìƒíƒœ: healthy/unhealthy
- ëª¨ë¸ íƒ€ì…
- íŠ¹ì„± ê°œìˆ˜

### POST /predict
ë‹¨ì¼ ì˜ˆì¸¡
```json
{{
  "feature1": 1.0,
  "feature2": 2.0,
  ...
}}
```

**ì‘ë‹µ**:
```json
{{
  "prediction": 1,
  "probability": [0.2, 0.8]
}}
```

### POST /batch_predict
ë°°ì¹˜ ì˜ˆì¸¡
```json
[
  {{"feature1": 1.0, "feature2": 2.0}},
  {{"feature1": 3.0, "feature2": 4.0}}
]
```

## í…ŒìŠ¤íŠ¸

```bash
# í—¬ìŠ¤ ì²´í¬
curl http://localhost:8000/health

# ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
curl -X POST "http://localhost:8000/predict" \\
  -H "Content-Type: application/json" \\
  -d '{{"feature1": 1.0, "feature2": 2.0}}'
```

## í”„ë¡œë•ì…˜ ë°°í¬

### í™˜ê²½ ë³€ìˆ˜
- `PORT`: API í¬íŠ¸ (ê¸°ë³¸ê°’: 8000)
- `WORKERS`: Uvicorn ì›Œì»¤ ìˆ˜ (ê¸°ë³¸ê°’: 1)

### ì„±ëŠ¥ íŠœë‹
```bash
uvicorn app:app --host 0.0.0.0 --port 8000 --workers 4
```

## ëª¨ë‹ˆí„°ë§

- Prometheus ë©”íŠ¸ë¦­: `/metrics` (ì¶”ê°€ ì„¤ì • í•„ìš”)
- ë¡œê·¸: stdout/stderr

## ë³´ì•ˆ

- API í‚¤ ì¸ì¦ êµ¬í˜„ ê¶Œì¥
- HTTPS ì‚¬ìš©
- Rate limiting ì„¤ì •

## ë¼ì´ì„ ìŠ¤

MIT License
'''

    readme_path = os.path.join(output_dir, 'README.md')
    with open(readme_path, 'w', encoding='utf-8') as f:
        f.write(readme_content)

    print(f"âœ“ README.md ì €ì¥: {readme_path}")

    return readme_path


def main():
    parser = argparse.ArgumentParser(description='ëª¨ë¸ ë°°í¬ API ìƒì„± ìŠ¤í¬ë¦½íŠ¸')
    parser.add_argument('--model-path', type=str, required=True,
                        help='í•™ìŠµëœ ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (.pkl)')
    parser.add_argument('--feature-names', type=str, default=None,
                        help='íŠ¹ì„± ì´ë¦„ (ì‰¼í‘œë¡œ êµ¬ë¶„, ì˜ˆ: V1,V2,V3)')
    parser.add_argument('--sample-data', type=str, default=None,
                        help='ìƒ˜í”Œ ë°ì´í„° ê²½ë¡œ (íŠ¹ì„± ì´ë¦„ ìë™ ì¶”ì¶œ)')
    parser.add_argument('--target-column', type=str, default=None,
                        help='íƒ€ê²Ÿ ì»¬ëŸ¼ëª… (ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš© ì‹œ)')
    parser.add_argument('--task-type', type=str, choices=['classification', 'regression', 'auto'],
                        default='auto', help='íƒœìŠ¤í¬ íƒ€ì…')
    parser.add_argument('--output-dir', type=str, default=None,
                        help='ì¶œë ¥ ë””ë ‰í† ë¦¬')

    args = parser.parse_args()

    print_header("ëª¨ë¸ ë°°í¬ API ìƒì„± ì‹œì‘")

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    if args.output_dir:
        output_dir = args.output_dir
    else:
        model_path = Path(args.model_path)
        if 'projects' in model_path.parts:
            project_idx = model_path.parts.index('projects')
            project_name = model_path.parts[project_idx + 1]
            output_dir = f"projects/{project_name}/deployment"
        else:
            output_dir = "deployment"

    os.makedirs(output_dir, exist_ok=True)
    print(f"âœ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")

    # ëª¨ë¸ ë¡œë“œ
    model = load_model(args.model_path)
    model_name = Path(args.model_path).stem

    # íƒœìŠ¤í¬ íƒ€ì… ì¶”ì •
    if args.task_type == 'auto':
        if hasattr(model, 'predict_proba'):
            task_type = 'classification'
        else:
            task_type = 'regression'
        print(f"\nâœ“ ìë™ íƒœìŠ¤í¬ íƒ€ì… ê°ì§€: {task_type}")
    else:
        task_type = args.task_type

    # íŠ¹ì„± ì´ë¦„ ì¶”ì¶œ
    if args.feature_names:
        feature_names = [f.strip() for f in args.feature_names.split(',')]
        print(f"\nâœ“ íŠ¹ì„± ì´ë¦„ (ìˆ˜ë™): {len(feature_names)}ê°œ")
    elif args.sample_data:
        feature_names = get_feature_names(args.sample_data, args.target_column)
    else:
        raise ValueError("--feature-names ë˜ëŠ” --sample-data ì¤‘ í•˜ë‚˜ëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤.")

    # FastAPI ì½”ë“œ ìƒì„±
    api_path = generate_api_code(args.model_path, feature_names, output_dir, task_type)

    # Dockerfile ìƒì„±
    dockerfile_path = generate_dockerfile(output_dir)

    # docker-compose.yml ìƒì„±
    compose_path = generate_docker_compose(output_dir)

    # requirements.txt ìƒì„±
    requirements_path = generate_requirements_txt(output_dir)

    # README.md ìƒì„±
    readme_path = generate_readme(output_dir, model_name)

    # ëª¨ë¸ ë³µì‚¬
    import shutil
    model_dest = os.path.join(output_dir, 'model.pkl')
    shutil.copy(args.model_path, model_dest)
    print(f"\nâœ“ ëª¨ë¸ ë³µì‚¬: {model_dest}")

    print_header("ëª¨ë¸ ë°°í¬ API ìƒì„± ì™„ë£Œ")
    print(f"\nğŸ“ ëª¨ë“  íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {output_dir}/")
    print(f"   - app.py: FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜")
    print(f"   - Dockerfile: Docker ì´ë¯¸ì§€ ë¹Œë“œ")
    print(f"   - docker-compose.yml: Docker Compose ì„¤ì •")
    print(f"   - requirements.txt: Python íŒ¨í‚¤ì§€")
    print(f"   - README.md: ì‚¬ìš© ê°€ì´ë“œ")
    print(f"   - model.pkl: í•™ìŠµëœ ëª¨ë¸")

    print(f"\nğŸš€ API ì‹¤í–‰:")
    print(f"   cd {output_dir}")
    print(f"   uvicorn app:app --reload --host 0.0.0.0 --port 8000")
    print(f"\n   API ë¬¸ì„œ: http://localhost:8000/docs")

    print(f"\nğŸ³ Docker ì‹¤í–‰:")
    print(f"   cd {output_dir}")
    print(f"   docker-compose up -d")

    return 0


if __name__ == '__main__':
    sys.exit(main())
