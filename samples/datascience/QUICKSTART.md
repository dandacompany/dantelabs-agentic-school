# ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ (Quick Start)

## 1ï¸âƒ£ í™˜ê²½ ì„¤ì •

### uv íŒ¨í‚¤ì§€ ë§¤ë‹ˆì € ì„¤ì¹˜ (ê¶Œì¥ âš¡)

```bash
# uv ì„¤ì¹˜ (í•œ ë²ˆë§Œ)
curl -LsSf https://astral.sh/uv/install.sh | sh
# ë˜ëŠ” macOS
brew install uv
```

### íŒ¨í‚¤ì§€ ì„¤ì¹˜

**ë°©ë²• 1: uv ì‚¬ìš© (ê¶Œì¥ - 10-100ë°° ë¹ ë¦„)**:
```bash
cd samples/datascience

# requirements.txtë¡œ í•œ ë²ˆì— ì„¤ì¹˜
uv pip install -r requirements.txt
```

**ë°©ë²• 2: pip ì‚¬ìš© (ê¸°ì¡´ ë°©ì‹)**:
```bash
cd samples/datascience

# requirements.txtë¡œ ì„¤ì¹˜
pip install -r requirements.txt

# ë˜ëŠ” ê°œë³„ ì„¤ì¹˜
pip install pandas numpy matplotlib seaborn plotly \
    scikit-learn xgboost lightgbm catboost optuna \
    imbalanced-learn shap lime ydata-profiling \
    jupyter notebook ipywidgets
```

**ğŸ’¡ íŒ**: uvë¥¼ ì‚¬ìš©í•˜ë©´ ydata-profiling ê°™ì€ ëŒ€ìš©ëŸ‰ íŒ¨í‚¤ì§€ë„ 10ì´ˆ ì•ˆì— ì„¤ì¹˜ë©ë‹ˆë‹¤!

---

## 2ï¸âƒ£ ë°ì´í„° í™•ì¸

### ë°ì´í„° ìœ„ì¹˜
```
samples/datascience/data/raw/creditcard.csv
```

### ê¸°ë³¸ ì •ë³´ í™•ì¸
```bash
cd samples/datascience
python -c "
import pandas as pd
df = pd.read_csv('data/raw/creditcard.csv')
print(f'ì „ì²´ ê±°ë˜: {len(df):,}ê±´')
print(f'ì‚¬ê¸° ê±°ë˜: {df[\"Class\"].sum():,}ê±´ ({df[\"Class\"].mean()*100:.2f}%)')
"
```

**ì¶œë ¥ ì˜ˆì‹œ**:
```
ì „ì²´ ê±°ë˜: 284,807ê±´
ì‚¬ê¸° ê±°ë˜: 492ê±´ (0.17%)
```

---

## 3ï¸âƒ£ ë°ì´í„° í”„ë¡œíŒŒì¼ë§

### ìë™í™”ëœ EDA ë¦¬í¬íŠ¸ ìƒì„±
```bash
python scripts/01_data_profiling.py
```

**ì¶œë ¥**:
- `outputs/reports/data_profile_report.html`
- ë¸Œë¼ìš°ì €ì—ì„œ ì—´ê¸°: `open outputs/reports/data_profile_report.html`

**í¬í•¨ ë‚´ìš©**:
- ê° ë³€ìˆ˜ë³„ ë¶„í¬, í†µê³„
- ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤
- ê²°ì¸¡ì¹˜ ë¶„ì„
- ì´ìƒì¹˜ íƒì§€
- ì‹œê³„ì—´ íŒ¨í„´

---

## 4ï¸âƒ£ ê°„ë‹¨í•œ ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸

### Python ìŠ¤í¬ë¦½íŠ¸ë¡œ ì‹¤í–‰
```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score

# ë°ì´í„° ë¡œë“œ
df = pd.read_csv('data/raw/creditcard.csv')
X = df.drop('Class', axis=1)
y = df['Class']

# Train/Test ë¶„ë¦¬ (stratified)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ (Logistic Regression)
model = LogisticRegression(class_weight='balanced', max_iter=1000)
model.fit(X_train, y_train)

# í‰ê°€
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]

print("=" * 60)
print("ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ ì„±ëŠ¥ (Logistic Regression)")
print("=" * 60)
print(classification_report(y_test, y_pred))
print(f"\nROC-AUC: {roc_auc_score(y_test, y_proba):.4f}")
```

**ì˜ˆìƒ ì¶œë ¥**:
```
              precision    recall  f1-score   support

           0       1.00      0.98      0.99     56864
           1       0.06      0.92      0.11        98

    accuracy                           0.98     56962
   macro avg       0.53      0.95      0.55     56962
weighted avg       1.00      0.98      0.99     56962

ROC-AUC: 0.9763
```

âš ï¸ **ì£¼ì˜**: Precisionì´ ë‚®ì€ ì´ìœ ëŠ” ë¶ˆê· í˜• ë•Œë¬¸. ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ê°œì„ !

---

## 5ï¸âƒ£ SMOTE ì ìš© ì˜ˆì‹œ

```python
from imblearn.over_sampling import SMOTE

# SMOTE ì ìš©
smote = SMOTE(random_state=42)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)

# ëª¨ë¸ ì¬í•™ìŠµ
model_smote = LogisticRegression(max_iter=1000)
model_smote.fit(X_train_balanced, y_train_balanced)

# í‰ê°€
y_pred_smote = model_smote.predict(X_test)
y_proba_smote = model_smote.predict_proba(X_test)[:, 1]

print("=" * 60)
print("SMOTE ì ìš© í›„ ì„±ëŠ¥")
print("=" * 60)
print(classification_report(y_test, y_pred_smote))
print(f"\nROC-AUC: {roc_auc_score(y_test, y_proba_smote):.4f}")
```

**ê°œì„  íš¨ê³¼**:
- Precision â¬†ï¸
- Recall ìœ ì§€
- F1-Score â¬†ï¸

---

## 6ï¸âƒ£ XGBoost ëª¨ë¸

```python
import xgboost as xgb

# í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¹„ìœ¨ ê³„ì‚°
scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()

# XGBoost ëª¨ë¸
xgb_model = xgb.XGBClassifier(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    scale_pos_weight=scale_pos_weight,
    random_state=42
)
xgb_model.fit(X_train, y_train)

# í‰ê°€
y_pred_xgb = xgb_model.predict(X_test)
y_proba_xgb = xgb_model.predict_proba(X_test)[:, 1]

print("=" * 60)
print("XGBoost ëª¨ë¸ ì„±ëŠ¥")
print("=" * 60)
print(classification_report(y_test, y_pred_xgb))
print(f"\nROC-AUC: {roc_auc_score(y_test, y_proba_xgb):.4f}")
```

---

## 7ï¸âƒ£ Jupyter ë…¸íŠ¸ë¶ìœ¼ë¡œ ì‹¤ìŠµ

```bash
# Jupyter Notebook ì‹¤í–‰
jupyter notebook notebooks/
```

**ì¶”ì²œ ì‹¤ìŠµ ìˆœì„œ**:
1. `01_exploratory_data_analysis.ipynb` - EDA
2. `02_feature_engineering.ipynb` - íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§
3. `03_imbalance_handling.ipynb` - ë¶ˆê· í˜• ì²˜ë¦¬
4. `04_modeling.ipynb` - ëª¨ë¸ í•™ìŠµ
5. `05_evaluation_and_interpretation.ipynb` - í‰ê°€ ë° í•´ì„

---

## 8ï¸âƒ£ ëª¨ë¸ ë¹„êµ ìŠ¤í¬ë¦½íŠ¸

```python
from sklearn.ensemble import RandomForestClassifier
import lightgbm as lgb

models = {
    "Logistic Regression": LogisticRegression(class_weight='balanced', max_iter=1000),
    "Random Forest": RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42),
    "XGBoost": xgb.XGBClassifier(scale_pos_weight=scale_pos_weight, random_state=42),
    "LightGBM": lgb.LGBMClassifier(is_unbalance=True, random_state=42)
}

results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    y_proba = model.predict_proba(X_test)[:, 1]
    auc = roc_auc_score(y_test, y_proba)
    results[name] = auc
    print(f"{name}: ROC-AUC = {auc:.4f}")

# ê²°ê³¼ ì •ë ¬
import pandas as pd
results_df = pd.DataFrame.from_dict(results, orient='index', columns=['ROC-AUC'])
results_df = results_df.sort_values('ROC-AUC', ascending=False)
print("\nëª¨ë¸ ì„±ëŠ¥ ìˆœìœ„:")
print(results_df)
```

---

## 9ï¸âƒ£ í‰ê°€ ì§€í‘œ ì‹œê°í™”

```python
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import precision_recall_curve
import matplotlib.pyplot as plt

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred_xgb)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['ì •ìƒ', 'ì‚¬ê¸°'])
disp.plot(cmap='Blues')
plt.title('Confusion Matrix - XGBoost')
plt.savefig('outputs/figures/confusion_matrix.png', dpi=300, bbox_inches='tight')
plt.show()

# Precision-Recall Curve
precision, recall, thresholds = precision_recall_curve(y_test, y_proba_xgb)
plt.figure(figsize=(10, 6))
plt.plot(recall, precision, marker='.')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve - XGBoost')
plt.grid(True)
plt.savefig('outputs/figures/pr_curve.png', dpi=300, bbox_inches='tight')
plt.show()
```

---

## ğŸ”Ÿ ë‹¤ìŒ ë‹¨ê³„

### ê³ ê¸‰ ê¸°ë²• ì ìš©
- [ ] SHAPìœ¼ë¡œ ëª¨ë¸ í•´ì„
- [ ] Optunaë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
- [ ] Isolation Forest (ì´ìƒíƒì§€)
- [ ] Autoencoder (ë”¥ëŸ¬ë‹ ì´ìƒíƒì§€)
- [ ] Threshold ìµœì í™” (ë¹„ìš© í•¨ìˆ˜ ê³ ë ¤)

### í”„ë¡œë•ì…˜ ë°°í¬
- [ ] FastAPIë¡œ REST API êµ¬ì¶•
- [ ] Docker ì»¨í…Œì´ë„ˆí™”
- [ ] ëª¨ë¸ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ
- [ ] A/B í…ŒìŠ¤íŠ¸ ì„¤ê³„

---

## ğŸ“š ì°¸ê³  ëª…ë ¹ì–´

```bash
# í”„ë¡œì íŠ¸ êµ¬ì¡° í™•ì¸
tree -L 2

# íŠ¹ì • ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
python scripts/01_data_profiling.py

# Jupyter ì‹¤í–‰
jupyter notebook

# íŒ¨í‚¤ì§€ ë²„ì „ í™•ì¸
pip list | grep -E "pandas|scikit|xgboost|lightgbm"
```

---

**ë¬¸ì œ ë°œìƒ ì‹œ**:
- ë°ì´í„°ê°€ ì—†ìœ¼ë©´: `README.md`ì˜ Kaggle ë‹¤ìš´ë¡œë“œ ì§€ì¹¨ í™•ì¸
- íŒ¨í‚¤ì§€ ì—ëŸ¬: ê°€ìƒí™˜ê²½ ìƒì„± í›„ ì¬ì„¤ì¹˜ ê¶Œì¥
- ë©”ëª¨ë¦¬ ë¶€ì¡±: ë°ì´í„° ìƒ˜í”Œë§ ì ìš© (`df.sample(50000)`)

**ìƒì„±ì¼**: 2026-01-31
