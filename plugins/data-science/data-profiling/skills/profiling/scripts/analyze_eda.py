#!/usr/bin/env python3
"""
EDA ë¶„ì„ ë ˆí¬íŠ¸ ìƒì„± ìŠ¤í¬ë¦½íŠ¸

í”„ë¡œíŒŒì¼ë§ ë¦¬í¬íŠ¸ì™€ ì›ë³¸ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ë°ì´í„° ì „ì²˜ë¦¬, ì¶”ê°€ ë¶„ì„, ëª¨ë¸ë§ ì§€ì¹¨ì„ ë‹´ì€
A4 í•œ ì¥ ë¶„ëŸ‰ì˜ ë ˆí¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

ì„¤ì¹˜:
    # uv ì‚¬ìš© (ê¶Œì¥)
    cd plugins/data-profiling/skills/profiling
    uv pip install --system -r requirements.txt

ì‚¬ìš©ë²•:
    python analyze_eda.py \
      --profile-path "./outputs/reports/creditcard_profile_report.html" \
      --data-path "./data/raw/creditcard.csv" \
      --target-column "Class"

    # PDF ì¶œë ¥ (pandoc í•„ìš”)
    python analyze_eda.py \
      --data-path "./data/raw/creditcard.csv" \
      --target-column "Class" \
      --output-format pdf

í•„ìš” íŒ¨í‚¤ì§€:
    - pandas
    - numpy
"""

import argparse
import os
import subprocess
from datetime import datetime
from pathlib import Path

import numpy as np
import pandas as pd


def analyze_dataset(df, target_column=None):
    """ë°ì´í„°ì…‹ ê¸°ë³¸ ë¶„ì„"""
    analysis = {
        'n_rows': len(df),
        'n_cols': len(df.columns),
        'memory_mb': df.memory_usage(deep=True).sum() / 1024**2,
        'n_missing': df.isnull().sum().sum(),
        'n_duplicates': df.duplicated().sum(),
    }

    # íƒ€ê²Ÿ ì»¬ëŸ¼ ë¶„ì„
    if target_column and target_column in df.columns:
        value_counts = df[target_column].value_counts()
        analysis['target_distribution'] = value_counts.to_dict()

        if len(value_counts) == 2:
            majority = value_counts.max()
            minority = value_counts.min()
            analysis['imbalance_ratio'] = majority / minority

    # ê²°ì¸¡ì¹˜ ë¶„ì„
    missing_cols = df.isnull().sum()
    analysis['missing_columns'] = missing_cols[missing_cols > 0].to_dict()

    # ìˆ˜ì¹˜í˜•/ë²”ì£¼í˜• ë¶„ë¦¬
    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()

    analysis['numeric_cols'] = numeric_cols
    analysis['categorical_cols'] = categorical_cols

    # ìŠ¤ì¼€ì¼ ì°¨ì´ ë¶„ì„
    if len(numeric_cols) > 1:
        scales = df[numeric_cols].std()
        scales = scales[scales > 0]  # 0 ì œê±°
        if len(scales) > 1:
            analysis['scale_ratio'] = scales.max() / scales.min()

    # ìƒê´€ê´€ê³„ ë¶„ì„
    if len(numeric_cols) > 1:
        corr_matrix = df[numeric_cols].corr().abs()
        # ëŒ€ê°ì„  ì œì™¸
        corr_matrix = corr_matrix.where(
            ~np.triu(np.ones(corr_matrix.shape)).astype(bool)
        )
        high_corr = corr_matrix[corr_matrix > 0.9].stack()
        analysis['high_corr_pairs'] = len(high_corr)

    return analysis


def detect_problem_type(df, target_column):
    """ë¬¸ì œ ìœ í˜• ìë™ ê°ì§€"""
    if target_column is None:
        return "unsupervised"

    if target_column not in df.columns:
        return "unknown"

    target_dtype = df[target_column].dtype
    n_unique = df[target_column].nunique()

    # ë¶„ë¥˜ vs íšŒê·€
    if target_dtype in ['int64', 'int32'] and n_unique <= 20:
        return "classification"
    elif target_dtype in ['object', 'category']:
        return "classification"
    elif target_dtype in ['float64', 'float32']:
        return "regression"
    else:
        return "unknown"


def generate_preprocessing_guide(analysis, problem_type):
    """ë°ì´í„° ì „ì²˜ë¦¬ ì§€ì¹¨ ìƒì„±"""
    guides = []

    # 1. ìŠ¤ì¼€ì¼ë§
    if analysis.get('scale_ratio', 0) > 100:
        guides.append({
            'priority': 'High',
            'title': 'ìŠ¤ì¼€ì¼ë§',
            'description': f"ë³€ìˆ˜ ê°„ ìŠ¤ì¼€ì¼ ì°¨ì´ê°€ í½ë‹ˆë‹¤ (ìµœëŒ€/ìµœì†Œ = {analysis['scale_ratio']:.0f}ë°°)",
            'code': """from sklearn.preprocessing import RobustScaler

# ì´ìƒì¹˜ì— ê°•ê±´í•œ RobustScaler ê¶Œì¥
scaler = RobustScaler()
X_scaled = scaler.fit_transform(X[numeric_cols])"""
        })

    # 2. í´ë˜ìŠ¤ ë¶ˆê· í˜•
    if problem_type == "classification" and analysis.get('imbalance_ratio', 0) > 10:
        ratio = analysis['imbalance_ratio']
        guides.append({
            'priority': 'Critical',
            'title': 'í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬',
            'description': f"ë¶ˆê· í˜• ë¹„ìœ¨ 1:{ratio:.0f}",
            'code': f"""from imblearn.over_sampling import SMOTE

# SMOTEë¡œ ì†Œìˆ˜ í´ë˜ìŠ¤ ì˜¤ë²„ìƒ˜í”Œë§
smote = SMOTE(sampling_strategy=0.1, random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# ë˜ëŠ” Class weights ì¡°ì •
from xgboost import XGBClassifier
model = XGBClassifier(scale_pos_weight={ratio:.0f})"""
        })

    # 3. ê²°ì¸¡ì¹˜
    if analysis['n_missing'] > 0:
        missing_pct = (analysis['n_missing'] / (analysis['n_rows'] * analysis['n_cols'])) * 100
        guides.append({
            'priority': 'Medium',
            'title': 'ê²°ì¸¡ì¹˜ ì²˜ë¦¬',
            'description': f"ì „ì²´ ë°ì´í„°ì˜ {missing_pct:.2f}%ê°€ ê²°ì¸¡",
            'code': """from sklearn.impute import SimpleImputer

# ìˆ˜ì¹˜í˜•: ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´
imputer_num = SimpleImputer(strategy='median')
X[numeric_cols] = imputer_num.fit_transform(X[numeric_cols])

# ë²”ì£¼í˜•: ìµœë¹ˆê°’ìœ¼ë¡œ ëŒ€ì²´
imputer_cat = SimpleImputer(strategy='most_frequent')
X[cat_cols] = imputer_cat.fit_transform(X[cat_cols])"""
        })

    return guides


def generate_analysis_recommendations(df, analysis, problem_type):
    """ì¶”ê°€ ë¶„ì„ ê¶Œê³ ì‚¬í•­ ìƒì„±"""
    recommendations = []

    # 1. Feature Importance
    if problem_type in ['classification', 'regression']:
        recommendations.append({
            'title': 'Feature Importance ë¶„ì„',
            'description': 'ì¤‘ìš” ë³€ìˆ˜ ì‹ë³„ ë° ì°¨ì› ì¶•ì†Œ',
            'code': """import xgboost as xgb

model = xgb.XGBClassifier()
model.fit(X_train, y_train)

# ë³€ìˆ˜ ì¤‘ìš”ë„ ì‹œê°í™”
xgb.plot_importance(model, max_num_features=15)
plt.tight_layout()
plt.show()

# ìƒìœ„ ë³€ìˆ˜ë§Œ ì„ íƒ
from sklearn.feature_selection import SelectFromModel
selector = SelectFromModel(model, prefit=True, threshold='median')
X_selected = selector.transform(X)"""
        })

    # 2. ì‹œê°„ ë³€ìˆ˜ ë¶„ì„ (Time ì»¬ëŸ¼ ì¡´ì¬ ì‹œ)
    if 'Time' in df.columns or 'time' in df.columns:
        recommendations.append({
            'title': 'ì‹œê°„ íŠ¹ì„± ì¶”ì¶œ',
            'description': 'Time ë³€ìˆ˜ì—ì„œ ìœ ìš©í•œ íŒŒìƒ ë³€ìˆ˜ ìƒì„±',
            'code': """# ì‹œê°„ëŒ€ ì¶”ì¶œ
X['Hour'] = (X['Time'] / 3600) % 24
X['Day'] = (X['Time'] / 86400).astype(int)

# ì£¼ê¸°ì„± ì¸ì½”ë”© (Cyclical encoding)
X['Hour_sin'] = np.sin(2 * np.pi * X['Hour'] / 24)
X['Hour_cos'] = np.cos(2 * np.pi * X['Hour'] / 24)

# ì‹œê°„ëŒ€ë³„ íŒ¨í„´ ë¶„ì„
fraud_by_hour = df.groupby('Hour')['Class'].mean()
fraud_by_hour.plot(kind='bar', title='Target Rate by Hour')"""
        })

    # 3. SHAP ë¶„ì„
    if problem_type in ['classification', 'regression']:
        recommendations.append({
            'title': 'SHAP ë¶„ì„ (ëª¨ë¸ í•´ì„)',
            'description': 'ì˜ˆì¸¡ì— ê¸°ì—¬í•˜ëŠ” ë³€ìˆ˜ì™€ ë°©í–¥ì„± ì´í•´',
            'code': """import shap

# Tree ê¸°ë°˜ ëª¨ë¸ìš©
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

# Summary plot
shap.summary_plot(shap_values, X_test)

# Force plot (ê°œë³„ ì˜ˆì¸¡ ì„¤ëª…)
shap.force_plot(explainer.expected_value, shap_values[0], X_test.iloc[0])"""
        })

    # 4. ìƒê´€ê´€ê³„ ë¶„ì„
    if analysis.get('high_corr_pairs', 0) > 0:
        recommendations.append({
            'title': 'ë‹¤ì¤‘ê³µì„ ì„± ì œê±°',
            'description': f"ë†’ì€ ìƒê´€ê´€ê³„ ë³€ìˆ˜ ìŒ: {analysis['high_corr_pairs']}ê°œ",
            'code': """# ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤
corr_matrix = X.corr().abs()

# ë†’ì€ ìƒê´€ê´€ê³„ ë³€ìˆ˜ ì°¾ê¸° (>0.9)
high_corr = (corr_matrix > 0.9).sum()
vars_to_drop = high_corr[high_corr > 1].index

# ì œê±°
X_reduced = X.drop(columns=vars_to_drop)"""
        })

    return recommendations


def generate_modeling_strategy(problem_type, analysis):
    """ëª¨ë¸ë§ ì „ëµ ìƒì„±"""
    strategy = {
        'algorithms': [],
        'metrics': [],
        'cv_strategy': None,
        'hyperparameters': []
    }

    if problem_type == 'classification':
        # ì•Œê³ ë¦¬ì¦˜ ì¶”ì²œ
        strategy['algorithms'] = [
            {
                'rank': 1,
                'name': 'XGBoost',
                'reason': 'ë¶ˆê· í˜• ë°ì´í„° ê°•ì , Feature importance',
                'code': """from xgboost import XGBClassifier

model = XGBClassifier(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    scale_pos_weight=578,  # ë¶ˆê· í˜• ë¹„ìœ¨
    random_state=42
)
model.fit(X_train, y_train)"""
            },
            {
                'rank': 2,
                'name': 'LightGBM',
                'reason': 'ë¹ ë¥¸ í•™ìŠµ ì†ë„, ëŒ€ìš©ëŸ‰ ë°ì´í„° íš¨ìœ¨',
                'code': """from lightgbm import LGBMClassifier

model = LGBMClassifier(
    n_estimators=100,
    is_unbalance=True,  # ë¶ˆê· í˜• ìë™ ì²˜ë¦¬
    random_state=42
)
model.fit(X_train, y_train)"""
            },
            {
                'rank': 3,
                'name': 'Random Forest',
                'reason': 'ì•ˆì •ì  ì„±ëŠ¥, í•´ì„ ê°€ëŠ¥',
                'code': """from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(
    n_estimators=100,
    class_weight='balanced',
    random_state=42
)
model.fit(X_train, y_train)"""
            }
        ]

        # í‰ê°€ ì§€í‘œ
        if analysis.get('imbalance_ratio', 0) > 10:
            strategy['metrics'] = [
                '**F1-Score** (Precision-Recall ê· í˜•)',
                '**PR-AUC** (ë¶ˆê· í˜• ë°ì´í„° ìµœì )',
                '**Recall** (False Negative ë¹„ìš© ë†’ìŒ)',
                '**Precision** (False Positive ë¹„ìš© ë†’ìŒ)',
                'âš ï¸ Accuracy ì‚¬ìš© ê¸ˆì§€ (ë¶ˆê· í˜•ìœ¼ë¡œ ë¬´ì˜ë¯¸)'
            ]
        else:
            strategy['metrics'] = [
                '**Accuracy**',
                '**F1-Score**',
                '**ROC-AUC**'
            ]

        # CV ì „ëµ
        strategy['cv_strategy'] = """from sklearn.model_selection import StratifiedKFold

# í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€í•˜ë©° 5-fold CV
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"""

    elif problem_type == 'regression':
        strategy['algorithms'] = [
            {'rank': 1, 'name': 'XGBoost Regressor', 'reason': 'ë†’ì€ ì„±ëŠ¥'},
            {'rank': 2, 'name': 'Random Forest Regressor', 'reason': 'ì•ˆì •ì„±'},
            {'rank': 3, 'name': 'LightGBM Regressor', 'reason': 'ì†ë„'}
        ]
        strategy['metrics'] = ['RMSE', 'MAE', 'R-squared']
        strategy['cv_strategy'] = 'KFold(n_splits=5)'

    return strategy


def generate_markdown_report(df, analysis, problem_type, target_column, dataset_name):
    """Markdown ë ˆí¬íŠ¸ ìƒì„±"""

    # ì „ì²˜ë¦¬ ê°€ì´ë“œ
    preprocessing_guides = generate_preprocessing_guide(analysis, problem_type)

    # ì¶”ê°€ ë¶„ì„ ê¶Œê³ ì‚¬í•­
    recommendations = generate_analysis_recommendations(df, analysis, problem_type)

    # ëª¨ë¸ë§ ì „ëµ
    modeling = generate_modeling_strategy(problem_type, analysis)

    # Executive Summary ìƒì„±
    exec_summary = []
    if analysis.get('imbalance_ratio', 0) > 10:
        exec_summary.append(f"ê·¹ì‹¬í•œ í´ë˜ìŠ¤ ë¶ˆê· í˜• (1:{analysis['imbalance_ratio']:.0f})")
    if analysis.get('scale_ratio', 0) > 100:
        exec_summary.append(f"ë³€ìˆ˜ ìŠ¤ì¼€ì¼ ì°¨ì´ ({analysis['scale_ratio']:.0f}ë°°)")
    if 'Time' in df.columns:
        exec_summary.append("ì‹œê°„ ë³€ìˆ˜ í™œìš© ê°€ëŠ¥")

    # Markdown ìƒì„±
    report = f"""# EDA ë¶„ì„ ë¦¬í¬íŠ¸: {dataset_name}

**ìƒì„±ì¼**: {datetime.now().strftime('%Y-%m-%d %H:%M')}
**ë¶„ì„ ëŒ€ìƒ**: {dataset_name} ({analysis['n_rows']:,}ê±´)
**ë¬¸ì œ ìœ í˜•**: {problem_type.title()}

---

## ğŸ“Š Executive Summary

"""

    if exec_summary:
        for item in exec_summary:
            report += f"- {item}\n"
    else:
        report += "- ë°ì´í„° í’ˆì§ˆ ì–‘í˜¸\n- í‘œì¤€ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì ìš© ê°€ëŠ¥\n"

    report += f"""
---

## ğŸ“‹ ë°ì´í„° ê°œìš”

| í•­ëª© | ê°’ |
|------|-----|
| ì „ì²´ ê±´ìˆ˜ | {analysis['n_rows']:,}ê±´ |
| íŠ¹ì„± ê°œìˆ˜ | {analysis['n_cols']}ê°œ |
| ê²°ì¸¡ì¹˜ | {analysis['n_missing']:,}ê°œ |
| ì¤‘ë³µ | {analysis['n_duplicates']:,}ê±´ |
| ë©”ëª¨ë¦¬ | {analysis['memory_mb']:.1f} MB |
| ìˆ˜ì¹˜í˜• ë³€ìˆ˜ | {len(analysis['numeric_cols'])}ê°œ |
| ë²”ì£¼í˜• ë³€ìˆ˜ | {len(analysis['categorical_cols'])}ê°œ |
"""

    # íƒ€ê²Ÿ ë¶„í¬
    if target_column and 'target_distribution' in analysis:
        report += f"\n**íƒ€ê²Ÿ ë¶„í¬** (`{target_column}`):\n"
        for cls, count in analysis['target_distribution'].items():
            pct = count / analysis['n_rows'] * 100
            report += f"- í´ë˜ìŠ¤ {cls}: {count:,}ê±´ ({pct:.2f}%)\n"

        if 'imbalance_ratio' in analysis:
            report += f"- ë¶ˆê· í˜• ë¹„ìœ¨: **1:{analysis['imbalance_ratio']:.0f}** âš ï¸\n"

    report += "\n---\n\n## ğŸ” ì£¼ìš” ë°œê²¬ì‚¬í•­\n\n"

    # ì£¼ìš” ì´ìŠˆ
    findings = []
    if analysis.get('imbalance_ratio', 0) > 10:
        findings.append({
            'severity': 'Critical',
            'title': 'í´ë˜ìŠ¤ ë¶ˆê· í˜•',
            'description': f"ì‚¬ê¸° ê±°ë˜ê°€ ì „ì²´ì˜ {100/analysis['imbalance_ratio']:.2f}%ì— ë¶ˆê³¼í•©ë‹ˆë‹¤. Accuracy ì§€í‘œëŠ” ë¬´ì˜ë¯¸í•˜ë©°, Precision-Recall ì¤‘ì‹¬ í‰ê°€ê°€ í•„ìš”í•©ë‹ˆë‹¤."
        })

    if analysis.get('scale_ratio', 0) > 100:
        findings.append({
            'severity': 'High',
            'title': 'ë³€ìˆ˜ ìŠ¤ì¼€ì¼ ì°¨ì´',
            'description': f"ë³€ìˆ˜ ê°„ ìŠ¤ì¼€ì¼ ì°¨ì´ê°€ {analysis['scale_ratio']:.0f}ë°°ì…ë‹ˆë‹¤. ìŠ¤ì¼€ì¼ë§ í•„ìˆ˜ì…ë‹ˆë‹¤."
        })

    if analysis['n_missing'] > 0:
        missing_pct = (analysis['n_missing'] / (analysis['n_rows'] * analysis['n_cols'])) * 100
        findings.append({
            'severity': 'Medium',
            'title': 'ê²°ì¸¡ì¹˜ ì¡´ì¬',
            'description': f"ì „ì²´ ë°ì´í„°ì˜ {missing_pct:.2f}%ê°€ ê²°ì¸¡ì¹˜ì…ë‹ˆë‹¤."
        })

    for idx, finding in enumerate(findings, 1):
        report += f"### {idx}. {finding['title']} ({finding['severity']})\n{finding['description']}\n\n"

    report += "---\n\n## ğŸ“‹ ë°ì´í„° ì „ì²˜ë¦¬ ì§€ì¹¨\n\n"

    for guide in preprocessing_guides:
        report += f"### {guide['priority']} Priority: {guide['title']}\n\n"
        report += f"{guide['description']}\n\n"
        report += f"```python\n{guide['code']}\n```\n\n"

    report += "---\n\n## ğŸ” ì¶”ê°€ ë¶„ì„ ê¶Œê³ ì‚¬í•­\n\n"

    for idx, rec in enumerate(recommendations, 1):
        report += f"### {idx}. {rec['title']}\n\n"
        report += f"{rec['description']}\n\n"
        report += f"```python\n{rec['code']}\n```\n\n"

    report += "---\n\n## ğŸ¤– ëª¨ë¸ë§ ì „ëµ\n\n"

    # ì•Œê³ ë¦¬ì¦˜ ì¶”ì²œ
    if modeling['algorithms']:
        report += "### ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜\n\n"
        for algo in modeling['algorithms']:
            report += f"**{algo['rank']}ìˆœìœ„: {algo['name']}**\n"
            report += f"- ì„ íƒ ì´ìœ : {algo['reason']}\n"
            if 'code' in algo:
                report += f"\n```python\n{algo['code']}\n```\n"
            report += "\n"

    # í‰ê°€ ì§€í‘œ
    if modeling['metrics']:
        report += "### í‰ê°€ ì§€í‘œ\n\n"
        for metric in modeling['metrics']:
            report += f"- {metric}\n"
        report += "\n"

    # CV ì „ëµ
    if modeling['cv_strategy']:
        report += f"### êµì°¨ ê²€ì¦\n\n```python\n{modeling['cv_strategy']}\n```\n\n"

    report += """---

## ğŸ“Œ ë‹¤ìŒ ë‹¨ê³„ (Next Steps)

### ìš°ì„ ìˆœìœ„ 1 (ì¦‰ì‹œ ì‹¤í–‰)
"""

    if preprocessing_guides:
        report += f"- [ ] ë°ì´í„° ì „ì²˜ë¦¬: `/engineer-features`\n"

    if problem_type == 'classification' and analysis.get('imbalance_ratio', 0) > 10:
        report += "- [ ] í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬: `/handle-imbalance --method smote`\n"

    report += "- [ ] ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ í•™ìŠµ: `/train-models --algorithms xgboost`\n"

    report += """
### ìš°ì„ ìˆœìœ„ 2 (ëª¨ë¸ í•™ìŠµ í›„)
- [ ] Feature importance ë¶„ì„
- [ ] SHAP ë¶„ì„ìœ¼ë¡œ ëª¨ë¸ í•´ì„
- [ ] Threshold ìµœì í™”

### ìš°ì„ ìˆœìœ„ 3 (ì„±ëŠ¥ ê°œì„ )
- [ ] í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (Optuna)
- [ ] Ensemble ëª¨ë¸
- [ ] ì¶”ê°€ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§

---

**ìƒì„± ë„êµ¬**: data-profiling plugin v1.0.0
**ë‹¤ìŒ ì»¤ë§¨ë“œ**: `/engineer-features`, `/handle-imbalance`, `/train-models`
"""

    return report


def main():
    parser = argparse.ArgumentParser(
        description='EDA ë¶„ì„ ë ˆí¬íŠ¸ ìƒì„±',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    parser.add_argument(
        '--profile-path',
        type=str,
        help='í”„ë¡œíŒŒì¼ë§ HTML ë¦¬í¬íŠ¸ ê²½ë¡œ (ì„ íƒ)'
    )
    parser.add_argument(
        '--data-path',
        type=str,
        required=True,
        help='ì›ë³¸ ë°ì´í„° íŒŒì¼ ê²½ë¡œ'
    )
    parser.add_argument(
        '--target-column',
        type=str,
        help='íƒ€ê²Ÿ ë³€ìˆ˜ ì»¬ëŸ¼ëª…'
    )
    parser.add_argument(
        '--output-format',
        type=str,
        choices=['markdown', 'pdf'],
        default='markdown',
        help='ì¶œë ¥ í˜•ì‹ (ê¸°ë³¸ê°’: markdown)'
    )
    parser.add_argument(
        '--output-dir',
        type=str,
        default='outputs/reports',
        help='ë¦¬í¬íŠ¸ ì €ì¥ ë””ë ‰í† ë¦¬'
    )

    args = parser.parse_args()

    print("=" * 60)
    print("EDA ë¶„ì„ ì‹œì‘")
    print("=" * 60)

    # ë°ì´í„° ë¡œë“œ
    print(f"\në°ì´í„° ë¡œë“œ ì¤‘: {args.data_path}")
    df = pd.read_csv(args.data_path)
    print(f"âœ“ ì™„ë£Œ: {len(df):,}ê±´, {len(df.columns)}ê°œ ì»¬ëŸ¼")

    # ë¶„ì„ ìˆ˜í–‰
    print("\në¶„ì„ ìˆ˜í–‰ ì¤‘...")
    analysis = analyze_dataset(df, args.target_column)
    problem_type = detect_problem_type(df, args.target_column)

    print(f"âœ“ ë¬¸ì œ ìœ í˜•: {problem_type.title()}")

    # ë ˆí¬íŠ¸ ìƒì„±
    dataset_name = Path(args.data_path).stem
    report = generate_markdown_report(
        df, analysis, problem_type, args.target_column, dataset_name
    )

    # ì €ì¥
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    md_path = output_dir / f"{dataset_name}_eda_report.md"
    with open(md_path, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"\nâœ“ Markdown ë ˆí¬íŠ¸ ì €ì¥: {md_path}")

    # PDF ë³€í™˜ (pandoc ì‚¬ìš©)
    if args.output_format == 'pdf':
        pdf_path = output_dir / f"{dataset_name}_eda_report.pdf"
        try:
            subprocess.run([
                'pandoc', str(md_path),
                '-o', str(pdf_path),
                '--pdf-engine=xelatex',
                '-V', 'geometry:margin=1in'
            ], check=True)
            print(f"âœ“ PDF ë ˆí¬íŠ¸ ì €ì¥: {pdf_path}")
        except (subprocess.CalledProcessError, FileNotFoundError):
            print("âš ï¸  pandocì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ PDF ë³€í™˜ ì‹¤íŒ¨")
            print("   ì„¤ì¹˜: brew install pandoc")

    # ìš”ì•½ ì¶œë ¥
    print(f"\n{'=' * 60}")
    print("EDA ë¶„ì„ ì™„ë£Œ")
    print(f"{'=' * 60}")
    print(f"\nğŸ“Š ë°ì´í„°ì…‹: {dataset_name} ({analysis['n_rows']:,}ê±´)")

    if analysis.get('imbalance_ratio'):
        print(f"\nâš ï¸  í´ë˜ìŠ¤ ë¶ˆê· í˜•: 1:{analysis['imbalance_ratio']:.0f}")

    if analysis.get('scale_ratio'):
        print(f"âš ï¸  ìŠ¤ì¼€ì¼ ì°¨ì´: {analysis['scale_ratio']:.0f}ë°°")

    print(f"\nğŸ“ ë ˆí¬íŠ¸: {md_path}")
    print(f"\në‹¤ìŒ ë‹¨ê³„:")
    print("   /engineer-features")
    if problem_type == 'classification' and analysis.get('imbalance_ratio', 0) > 10:
        print("   /handle-imbalance")
    print("   /train-models\n")


if __name__ == "__main__":
    main()
