# EDA ë¶„ì„ ë¦¬í¬íŠ¸: creditcard

**ìƒì„±ì¼**: 2026-01-31 08:08
**ë¶„ì„ ëŒ€ìƒ**: creditcard (284,807ê±´)
**ë¬¸ì œ ìœ í˜•**: Classification

---

## ğŸ“Š Executive Summary

- ê·¹ì‹¬í•œ í´ë˜ìŠ¤ ë¶ˆê· í˜• (1:578)
- ë³€ìˆ˜ ìŠ¤ì¼€ì¼ ì°¨ì´ (1143543ë°°)
- ì‹œê°„ ë³€ìˆ˜ í™œìš© ê°€ëŠ¥

---

## ğŸ“‹ ë°ì´í„° ê°œìš”

| í•­ëª© | ê°’ |
|------|-----|
| ì „ì²´ ê±´ìˆ˜ | 284,807ê±´ |
| íŠ¹ì„± ê°œìˆ˜ | 31ê°œ |
| ê²°ì¸¡ì¹˜ | 0ê°œ |
| ì¤‘ë³µ | 1,081ê±´ |
| ë©”ëª¨ë¦¬ | 67.4 MB |
| ìˆ˜ì¹˜í˜• ë³€ìˆ˜ | 31ê°œ |
| ë²”ì£¼í˜• ë³€ìˆ˜ | 0ê°œ |

**íƒ€ê²Ÿ ë¶„í¬** (`Class`):
- í´ë˜ìŠ¤ 0: 284,315ê±´ (99.83%)
- í´ë˜ìŠ¤ 1: 492ê±´ (0.17%)
- ë¶ˆê· í˜• ë¹„ìœ¨: **1:578** âš ï¸

---

## ğŸ” ì£¼ìš” ë°œê²¬ì‚¬í•­

### 1. í´ë˜ìŠ¤ ë¶ˆê· í˜• (Critical)
ì‚¬ê¸° ê±°ë˜ê°€ ì „ì²´ì˜ 0.17%ì— ë¶ˆê³¼í•©ë‹ˆë‹¤. Accuracy ì§€í‘œëŠ” ë¬´ì˜ë¯¸í•˜ë©°, Precision-Recall ì¤‘ì‹¬ í‰ê°€ê°€ í•„ìš”í•©ë‹ˆë‹¤.

### 2. ë³€ìˆ˜ ìŠ¤ì¼€ì¼ ì°¨ì´ (High)
ë³€ìˆ˜ ê°„ ìŠ¤ì¼€ì¼ ì°¨ì´ê°€ 1143543ë°°ì…ë‹ˆë‹¤. ìŠ¤ì¼€ì¼ë§ í•„ìˆ˜ì…ë‹ˆë‹¤.

---

## ğŸ“‹ ë°ì´í„° ì „ì²˜ë¦¬ ì§€ì¹¨

### High Priority: ìŠ¤ì¼€ì¼ë§

ë³€ìˆ˜ ê°„ ìŠ¤ì¼€ì¼ ì°¨ì´ê°€ í½ë‹ˆë‹¤ (ìµœëŒ€/ìµœì†Œ = 1143543ë°°)

```python
from sklearn.preprocessing import RobustScaler

# ì´ìƒì¹˜ì— ê°•ê±´í•œ RobustScaler ê¶Œì¥
scaler = RobustScaler()
X_scaled = scaler.fit_transform(X[numeric_cols])
```

### Critical Priority: í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬

ë¶ˆê· í˜• ë¹„ìœ¨ 1:578

```python
from imblearn.over_sampling import SMOTE

# SMOTEë¡œ ì†Œìˆ˜ í´ë˜ìŠ¤ ì˜¤ë²„ìƒ˜í”Œë§
smote = SMOTE(sampling_strategy=0.1, random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# ë˜ëŠ” Class weights ì¡°ì •
from xgboost import XGBClassifier
model = XGBClassifier(scale_pos_weight=578)
```

---

## ğŸ” ì¶”ê°€ ë¶„ì„ ê¶Œê³ ì‚¬í•­

### 1. Feature Importance ë¶„ì„

ì¤‘ìš” ë³€ìˆ˜ ì‹ë³„ ë° ì°¨ì› ì¶•ì†Œ

```python
import xgboost as xgb

model = xgb.XGBClassifier()
model.fit(X_train, y_train)

# ë³€ìˆ˜ ì¤‘ìš”ë„ ì‹œê°í™”
xgb.plot_importance(model, max_num_features=15)
plt.tight_layout()
plt.show()

# ìƒìœ„ ë³€ìˆ˜ë§Œ ì„ íƒ
from sklearn.feature_selection import SelectFromModel
selector = SelectFromModel(model, prefit=True, threshold='median')
X_selected = selector.transform(X)
```

### 2. ì‹œê°„ íŠ¹ì„± ì¶”ì¶œ

Time ë³€ìˆ˜ì—ì„œ ìœ ìš©í•œ íŒŒìƒ ë³€ìˆ˜ ìƒì„±

```python
# ì‹œê°„ëŒ€ ì¶”ì¶œ
X['Hour'] = (X['Time'] / 3600) % 24
X['Day'] = (X['Time'] / 86400).astype(int)

# ì£¼ê¸°ì„± ì¸ì½”ë”© (Cyclical encoding)
X['Hour_sin'] = np.sin(2 * np.pi * X['Hour'] / 24)
X['Hour_cos'] = np.cos(2 * np.pi * X['Hour'] / 24)

# ì‹œê°„ëŒ€ë³„ íŒ¨í„´ ë¶„ì„
fraud_by_hour = df.groupby('Hour')['Class'].mean()
fraud_by_hour.plot(kind='bar', title='Target Rate by Hour')
```

### 3. SHAP ë¶„ì„ (ëª¨ë¸ í•´ì„)

ì˜ˆì¸¡ì— ê¸°ì—¬í•˜ëŠ” ë³€ìˆ˜ì™€ ë°©í–¥ì„± ì´í•´

```python
import shap

# Tree ê¸°ë°˜ ëª¨ë¸ìš©
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

# Summary plot
shap.summary_plot(shap_values, X_test)

# Force plot (ê°œë³„ ì˜ˆì¸¡ ì„¤ëª…)
shap.force_plot(explainer.expected_value, shap_values[0], X_test.iloc[0])
```

---

## ğŸ¤– ëª¨ë¸ë§ ì „ëµ

### ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜

**1ìˆœìœ„: XGBoost**
- ì„ íƒ ì´ìœ : ë¶ˆê· í˜• ë°ì´í„° ê°•ì , Feature importance

```python
from xgboost import XGBClassifier

model = XGBClassifier(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    scale_pos_weight=578,  # ë¶ˆê· í˜• ë¹„ìœ¨
    random_state=42
)
model.fit(X_train, y_train)
```

**2ìˆœìœ„: LightGBM**
- ì„ íƒ ì´ìœ : ë¹ ë¥¸ í•™ìŠµ ì†ë„, ëŒ€ìš©ëŸ‰ ë°ì´í„° íš¨ìœ¨

```python
from lightgbm import LGBMClassifier

model = LGBMClassifier(
    n_estimators=100,
    is_unbalance=True,  # ë¶ˆê· í˜• ìë™ ì²˜ë¦¬
    random_state=42
)
model.fit(X_train, y_train)
```

**3ìˆœìœ„: Random Forest**
- ì„ íƒ ì´ìœ : ì•ˆì •ì  ì„±ëŠ¥, í•´ì„ ê°€ëŠ¥

```python
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(
    n_estimators=100,
    class_weight='balanced',
    random_state=42
)
model.fit(X_train, y_train)
```

### í‰ê°€ ì§€í‘œ

- **F1-Score** (Precision-Recall ê· í˜•)
- **PR-AUC** (ë¶ˆê· í˜• ë°ì´í„° ìµœì )
- **Recall** (False Negative ë¹„ìš© ë†’ìŒ)
- **Precision** (False Positive ë¹„ìš© ë†’ìŒ)
- âš ï¸ Accuracy ì‚¬ìš© ê¸ˆì§€ (ë¶ˆê· í˜•ìœ¼ë¡œ ë¬´ì˜ë¯¸)

### êµì°¨ ê²€ì¦

```python
from sklearn.model_selection import StratifiedKFold

# í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€í•˜ë©° 5-fold CV
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
```

---

## ğŸ“Œ ë‹¤ìŒ ë‹¨ê³„ (Next Steps)

### ìš°ì„ ìˆœìœ„ 1 (ì¦‰ì‹œ ì‹¤í–‰)
- [ ] ë°ì´í„° ì „ì²˜ë¦¬: `/engineer-features`
- [ ] í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬: `/handle-imbalance --method smote`
- [ ] ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ í•™ìŠµ: `/train-models --algorithms xgboost`

### ìš°ì„ ìˆœìœ„ 2 (ëª¨ë¸ í•™ìŠµ í›„)
- [ ] Feature importance ë¶„ì„
- [ ] SHAP ë¶„ì„ìœ¼ë¡œ ëª¨ë¸ í•´ì„
- [ ] Threshold ìµœì í™”

### ìš°ì„ ìˆœìœ„ 3 (ì„±ëŠ¥ ê°œì„ )
- [ ] í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (Optuna)
- [ ] Ensemble ëª¨ë¸
- [ ] ì¶”ê°€ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§

---

**ìƒì„± ë„êµ¬**: data-profiling plugin v1.0.0
**ë‹¤ìŒ ì»¤ë§¨ë“œ**: `/engineer-features`, `/handle-imbalance`, `/train-models`
