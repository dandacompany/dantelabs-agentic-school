---
name: train-model
description: ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¡œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  í‰ê°€í•©ë‹ˆë‹¤.
arguments:
  - name: X-train-path
    description: Train íŠ¹ì„± ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    required: true
  - name: y-train-path
    description: Train íƒ€ê²Ÿ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    required: true
  - name: X-test-path
    description: Test íŠ¹ì„± ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    required: true
  - name: y-test-path
    description: Test íƒ€ê²Ÿ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    required: true
  - name: algorithm
    description: í•™ìŠµí•  ì•Œê³ ë¦¬ì¦˜ (xgboost, lightgbm, random_forest)
    required: false
    default: "xgboost"
  - name: tune
    description: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ í™œì„±í™” (true/false)
    required: false
    default: "false"
  - name: output-dir
    description: ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬
    required: false
    default: "projects/{project-name}/outputs/models"
---

# /train-model

ì „ì²˜ë¦¬ ë° ë¦¬ìƒ˜í”Œë§ ì™„ë£Œëœ ë°ì´í„°ë¡œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  í‰ê°€í•©ë‹ˆë‹¤.

## Usage

```bash
# XGBoost í•™ìŠµ (ê¸°ë³¸ê°’)
/train-model \
  --X-train-path "projects/creditcard-fraud-detection/data/processed/X_train_balanced.csv" \
  --y-train-path "projects/creditcard-fraud-detection/data/processed/y_train_balanced.csv" \
  --X-test-path "projects/creditcard-fraud-detection/data/processed/X_test.csv" \
  --y-test-path "projects/creditcard-fraud-detection/data/processed/y_test.csv"

# LightGBM ì‚¬ìš©
/train-model \
  --X-train-path "projects/my-project/data/processed/X_train_balanced.csv" \
  --y-train-path "projects/my-project/data/processed/y_train_balanced.csv" \
  --X-test-path "projects/my-project/data/processed/X_test.csv" \
  --y-test-path "projects/my-project/data/processed/y_test.csv" \
  --algorithm lightgbm

# Random Forest ì‚¬ìš©
/train-model \
  --X-train-path "projects/my-project/data/processed/X_train_balanced.csv" \
  --y-train-path "projects/my-project/data/processed/y_train_balanced.csv" \
  --X-test-path "projects/my-project/data/processed/X_test.csv" \
  --y-test-path "projects/my-project/data/processed/y_test.csv" \
  --algorithm random_forest

# í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ í¬í•¨ (ì˜ˆì •)
/train-model \
  --X-train-path "projects/my-project/data/processed/X_train_balanced.csv" \
  --y-train-path "projects/my-project/data/processed/y_train_balanced.csv" \
  --X-test-path "projects/my-project/data/processed/X_test.csv" \
  --y-test-path "projects/my-project/data/processed/y_test.csv" \
  --algorithm xgboost \
  --tune true

# ì¶œë ¥ ë””ë ‰í† ë¦¬ ì§€ì •
/train-model \
  --X-train-path "projects/my-project/data/processed/X_train_balanced.csv" \
  --y-train-path "projects/my-project/data/processed/y_train_balanced.csv" \
  --X-test-path "projects/my-project/data/processed/X_test.csv" \
  --y-test-path "projects/my-project/data/processed/y_test.csv" \
  --output-dir "projects/my-project/outputs/experiment_1"
```

## What This Command Does

### 1. ë°ì´í„° ë¡œë“œ ë° ê²€ì¦
- Train/Test ë°ì´í„° ë¡œë“œ
- Shape í™•ì¸ (íŠ¹ì„± ìˆ˜ ì¼ì¹˜ ì—¬ë¶€)
- í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸

### 2. ëª¨ë¸ í•™ìŠµ

#### XGBoost (ê¸°ë³¸ê°’, ê¶Œì¥)
```python
import xgboost as xgb

model = xgb.XGBClassifier(
    n_estimators=100,      # íŠ¸ë¦¬ ê°œìˆ˜
    max_depth=6,           # íŠ¸ë¦¬ ê¹Šì´
    learning_rate=0.1,     # í•™ìŠµë¥ 
    random_state=42,
    eval_metric='logloss'
)
model.fit(X_train, y_train)
```

**ì¥ì **:
- âœ… ë†’ì€ ì„±ëŠ¥
- âœ… ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬ ê°•ì  (`scale_pos_weight`)
- âœ… Feature importance ì œê³µ
- âœ… ì •ê·œí™” ë‚´ì¥

**ì‚¬ìš© ì‹œê¸°**: ëŒ€ë¶€ë¶„ì˜ ê²½ìš° (ê¸°ë³¸ ì¶”ì²œ)

#### LightGBM
```python
import lightgbm as lgb

model = lgb.LGBMClassifier(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    random_state=42,
    verbose=-1
)
model.fit(X_train, y_train)
```

**ì¥ì **:
- âœ… XGBoostë³´ë‹¤ ë¹ ë¦„ (ëŒ€ìš©ëŸ‰ ë°ì´í„°)
- âœ… ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
- âœ… ë²”ì£¼í˜• ë³€ìˆ˜ ì§ì ‘ ì²˜ë¦¬

**ì‚¬ìš© ì‹œê¸°**: ëŒ€ìš©ëŸ‰ ë°ì´í„° (100ë§Œ ê±´ ì´ìƒ)

#### Random Forest
```python
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    random_state=42,
    n_jobs=-1
)
model.fit(X_train, y_train)
```

**ì¥ì **:
- âœ… í•´ì„ ê°€ëŠ¥
- âœ… ì•ˆì •ì 
- âœ… ê³¼ì í•© ëœí•¨

**ì‚¬ìš© ì‹œê¸°**: ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸, í•´ì„ ì¤‘ìš” ì‹œ

### 3. ëª¨ë¸ í‰ê°€

#### Classification Report
```
              precision    recall  f1-score   support

           0       1.00      1.00      1.00     56864
           1       0.81      0.85      0.83        98

    accuracy                           1.00     56962
   macro avg       0.90      0.92      0.91     56962
weighted avg       1.00      1.00      1.00     56962
```

#### ROC-AUC & PR-AUC
```python
from sklearn.metrics import roc_auc_score, precision_recall_curve, auc

# ROC-AUC
roc_auc = roc_auc_score(y_test, y_proba)

# PR-AUC (ë¶ˆê· í˜• ë°ì´í„°ì— ë” ì í•©)
precision, recall, _ = precision_recall_curve(y_test, y_proba)
pr_auc = auc(recall, precision)
```

#### Confusion Matrix
```
                Predicted
              0        1
Actual 0  56,844      20    # TN=56844, FP=20
Actual 1      15      83    # FN=15, TP=83
```

**í•´ì„**:
- **TP (True Positive)**: 83 - ì‚¬ê¸°ë¥¼ ì‚¬ê¸°ë¡œ ì •í™•íˆ ì˜ˆì¸¡
- **TN (True Negative)**: 56,844 - ì •ìƒì„ ì •ìƒìœ¼ë¡œ ì •í™•íˆ ì˜ˆì¸¡
- **FP (False Positive)**: 20 - ì •ìƒì„ ì‚¬ê¸°ë¡œ ì˜¤íŒ (Type I Error)
- **FN (False Negative)**: 15 - ì‚¬ê¸°ë¥¼ ì •ìƒìœ¼ë¡œ ì˜¤íŒ (Type II Error) âš ï¸

### 4. ëª¨ë¸ ì €ì¥
```python
import joblib

joblib.dump(model, 'projects/{project-name}/outputs/models/xgboost_model.pkl')
```

## Output Structure

```
projects/{project-name}/outputs/models/
â”œâ”€â”€ xgboost_model.pkl           # í•™ìŠµëœ ëª¨ë¸
â””â”€â”€ preprocessing_pipeline.pkl  # ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (ì´ì „ ë‹¨ê³„ì—ì„œ ìƒì„±)
```

### ì½˜ì†” ì¶œë ¥
```
============================================================
ëª¨ë¸ í•™ìŠµ ì‹œì‘
============================================================

ë°ì´í„° ë¡œë“œ ì¤‘...
âœ“ Train: 250,196ê±´
âœ“ Test: 56,962ê±´

ëª¨ë¸ í•™ìŠµ ì¤‘ (ì•Œê³ ë¦¬ì¦˜: xgboost)...
âœ“ í•™ìŠµ ì™„ë£Œ

ëª¨ë¸ í‰ê°€ ì¤‘...

============================================================
ë¶„ë¥˜ ë¦¬í¬íŠ¸
============================================================
              precision    recall  f1-score   support

           0       1.00      1.00      1.00     56864
           1       0.81      0.85      0.83        98

    accuracy                           1.00     56962
   macro avg       0.90      0.92      0.91     56962
weighted avg       1.00      1.00      1.00     56962

ROC-AUC: 0.9760
PR-AUC: 0.8701

Confusion Matrix:
                Predicted
              0        1
Actual 0  56,844      20
Actual 1      15      83

âœ“ ëª¨ë¸ ì €ì¥: projects/creditcard-fraud-detection/outputs/models/xgboost_model.pkl

============================================================
ëª¨ë¸ í•™ìŠµ ì™„ë£Œ
============================================================

ğŸ“Š ìµœì¢… ì„±ëŠ¥:
   ROC-AUC: 0.9760
   PR-AUC: 0.8701
```

## Algorithm Comparison

| ì•Œê³ ë¦¬ì¦˜ | ì†ë„ | ì„±ëŠ¥ | ë©”ëª¨ë¦¬ | í•´ì„ì„± | ì¶”ì²œ ìˆœìœ„ |
|---------|------|------|--------|--------|----------|
| **XGBoost** | ë³´í†µ | ë§¤ìš° ìš°ìˆ˜ | ë³´í†µ | ì¤‘ê°„ | â­â­â­ |
| **LightGBM** | ë¹ ë¦„ | ë§¤ìš° ìš°ìˆ˜ | ìš°ìˆ˜ | ì¤‘ê°„ | â­â­ |
| **Random Forest** | ëŠë¦¼ | ìš°ìˆ˜ | ë‚˜ì¨ | ìš°ìˆ˜ | â­ |

## Evaluation Metrics Guide

### ë¶ˆê· í˜• ë°ì´í„° (ì‚¬ê¸° íƒì§€, ì´ìƒ íƒì§€)

| ì§€í‘œ | ì‚¬ìš© ì—¬ë¶€ | ì´ìœ  |
|------|----------|------|
| **Accuracy** | âŒ ê¸ˆì§€ | 99.83% ë¶ˆê· í˜•ì—ì„œ ë¬´ì˜ë¯¸ |
| **Precision** | âœ… ì¤‘ìš” | FP ë¹„ìš© ê³ ë ¤ |
| **Recall** | âœ… ë§¤ìš° ì¤‘ìš” | FN ë¹„ìš© ê³ ë ¤ (ì‚¬ê¸° ë†“ì¹˜ë©´ ì†ì‹¤) |
| **F1-Score** | âœ… í•µì‹¬ | Precision-Recall ê· í˜• |
| **PR-AUC** | âœ… ìµœì  | ë¶ˆê· í˜• ë°ì´í„° ìµœì  ì§€í‘œ |
| **ROC-AUC** | âš ï¸ ì°¸ê³  | PR-AUCë³´ë‹¤ ëœ ìœ ìš© |

### ê· í˜• ë°ì´í„° (ê³ ê° ì´íƒˆ, ë¶„ë¥˜)

| ì§€í‘œ | ì‚¬ìš© ì—¬ë¶€ |
|------|----------|
| **Accuracy** | âœ… ì‚¬ìš© ê°€ëŠ¥ |
| **F1-Score** | âœ… ê¶Œì¥ |
| **ROC-AUC** | âœ… ê¶Œì¥ |

## Examples

### Example 1: ì‹ ìš©ì¹´ë“œ ì‚¬ê¸° íƒì§€ (XGBoost)
```bash
/train-model \
  --X-train-path "projects/creditcard-fraud-detection/data/processed/X_train_balanced.csv" \
  --y-train-path "projects/creditcard-fraud-detection/data/processed/y_train_balanced.csv" \
  --X-test-path "projects/creditcard-fraud-detection/data/processed/X_test.csv" \
  --y-test-path "projects/creditcard-fraud-detection/data/processed/y_test.csv" \
  --algorithm xgboost
```

**ì˜ˆìƒ ì„±ëŠ¥**:
- ROC-AUC: 0.97+
- PR-AUC: 0.87+
- F1-Score: 0.83+

### Example 2: ê³ ê° ì´íƒˆ ì˜ˆì¸¡ (LightGBM)
```bash
/train-model \
  --X-train-path "projects/customer-churn/data/processed/X_train_balanced.csv" \
  --y-train-path "projects/customer-churn/data/processed/y_train_balanced.csv" \
  --X-test-path "projects/customer-churn/data/processed/X_test.csv" \
  --y-test-path "projects/customer-churn/data/processed/y_test.csv" \
  --algorithm lightgbm
```

### Example 3: ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ (Random Forest)
```bash
/train-model \
  --X-train-path "projects/my-project/data/processed/X_train_balanced.csv" \
  --y-train-path "projects/my-project/data/processed/y_train_balanced.csv" \
  --X-test-path "projects/my-project/data/processed/X_test.csv" \
  --y-test-path "projects/my-project/data/processed/y_test.csv" \
  --algorithm random_forest
```

## Model Loading & Prediction

### í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ
```python
import joblib

# ëª¨ë¸ ë¡œë“œ
model = joblib.load('projects/creditcard-fraud-detection/outputs/models/xgboost_model.pkl')

# ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ë¡œë“œ
scaler = joblib.load('projects/creditcard-fraud-detection/outputs/models/preprocessing_pipeline.pkl')

# ì‹ ê·œ ë°ì´í„° ì „ì²˜ë¦¬
X_new_scaled = scaler.transform(X_new)

# ì˜ˆì¸¡
y_pred = model.predict(X_new_scaled)
y_proba = model.predict_proba(X_new_scaled)[:, 1]
```

### Threshold ìµœì í™”
```python
from sklearn.metrics import precision_recall_curve
import numpy as np

# ìµœì  ì„ê³„ê°’ ì°¾ê¸° (F1-Score ìµœëŒ€í™”)
precision, recall, thresholds = precision_recall_curve(y_test, y_proba)
f1_scores = 2 * (precision * recall) / (precision + recall)
optimal_idx = np.argmax(f1_scores)
optimal_threshold = thresholds[optimal_idx]

print(f"ìµœì  ì„ê³„ê°’: {optimal_threshold:.3f}")
print(f"F1-Score: {f1_scores[optimal_idx]:.3f}")

# ì˜ˆì¸¡ ì‹œ ì ìš©
y_pred_optimized = (y_proba >= optimal_threshold).astype(int)
```

## Performance Tips

### ê³¼ì í•© ë°©ì§€
```python
# XGBoost
model = xgb.XGBClassifier(
    n_estimators=100,
    max_depth=6,           # ë„ˆë¬´ ê¹Šì§€ ì•Šê²Œ
    learning_rate=0.1,
    subsample=0.8,         # ìƒ˜í”Œ ë¹„ìœ¨
    colsample_bytree=0.8,  # íŠ¹ì„± ë¹„ìœ¨
    reg_alpha=0.1,         # L1 ì •ê·œí™”
    reg_lambda=1.0,        # L2 ì •ê·œí™”
)
```

### ëŒ€ìš©ëŸ‰ ë°ì´í„°
```python
# LightGBM ì‚¬ìš©
model = lgb.LGBMClassifier(
    n_estimators=100,
    learning_rate=0.1,
    num_leaves=31,        # íŠ¸ë¦¬ ë³µì¡ë„
    max_bin=255,          # íˆìŠ¤í† ê·¸ë¨ bin ìˆ˜
)
```

### ì„±ëŠ¥ í–¥ìƒ
```python
# Stratified K-Fold CV
from sklearn.model_selection import StratifiedKFold

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='f1')
print(f"CV F1-Score: {scores.mean():.3f} (+/- {scores.std():.3f})")
```

## Feature Importance Analysis

```python
import matplotlib.pyplot as plt
import xgboost as xgb

# XGBoost Feature Importance
xgb.plot_importance(model, max_num_features=20)
plt.tight_layout()
plt.savefig('projects/{project-name}/outputs/figures/feature_importance.png')

# ìƒìœ„ ë³€ìˆ˜ ì¶”ì¶œ
importances = model.feature_importances_
indices = np.argsort(importances)[::-1]
top_features = X_train.columns[indices[:20]]
print("ìƒìœ„ 20ê°œ ì¤‘ìš” ë³€ìˆ˜:")
for i, feat in enumerate(top_features):
    print(f"{i+1}. {feat}: {importances[indices[i]]:.4f}")
```

## Troubleshooting

### ë¬¸ì œ: "ValueError: Number of features mismatch"
- Train/Test íŠ¹ì„± ê°œìˆ˜ ë¶ˆì¼ì¹˜
- ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ë™ì¼í•˜ê²Œ ì ìš© í™•ì¸

### ë¬¸ì œ: ê³¼ì í•© (Train ì„±ëŠ¥ >> Test ì„±ëŠ¥)
```python
# max_depth ì¤„ì´ê¸°
model = xgb.XGBClassifier(max_depth=3)

# ì •ê·œí™” ê°•í™”
model = xgb.XGBClassifier(reg_alpha=1.0, reg_lambda=10.0)
```

### ë¬¸ì œ: ì €ì„±ëŠ¥ (F1-Score < 0.5)
- ë¦¬ìƒ˜í”Œë§ ë¹„ìœ¨ ì¡°ì • (`/balance-data --ratio 0.2`)
- ì „ì²˜ë¦¬ ì¬í™•ì¸ (`/engineer-features`)
- ì•Œê³ ë¦¬ì¦˜ ë³€ê²½ (Random Forest â†’ XGBoost)

### ë¬¸ì œ: ë©”ëª¨ë¦¬ ë¶€ì¡±
- LightGBM ì‚¬ìš©
- n_estimators ì¤„ì´ê¸° (100 â†’ 50)
- ì²­í¬ ë‹¨ìœ„ í•™ìŠµ

## Related Commands

- `/balance-data`: í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬ (í•™ìŠµ ì „ í•„ìˆ˜)
- `/engineer-features`: íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ (ì „ì²˜ë¦¬)
- `/profile-data`: ë°ì´í„° ë¶„ì„

## Agents Used

- `model-trainer` (í•„ìˆ˜): ëª¨ë¸ í•™ìŠµ ë° í‰ê°€

## Notes

âš ï¸ **ì£¼ì˜ì‚¬í•­**:
- Train/Test ë°ì´í„° ë¶„ë¦¬ í™•ì¸
- ë¶ˆê· í˜• ë°ì´í„°ëŠ” Accuracy ì§€í‘œ ê¸ˆì§€
- ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ê³¼ ëª¨ë¸ í•¨ê»˜ ì €ì¥

ğŸ’¡ **íŒ**:
- XGBoostë¶€í„° ì‹œì‘ (ê¸°ë³¸ ì¶”ì²œ)
- F1-Scoreë¡œ í‰ê°€ (Precision-Recall ê· í˜•)
- Feature importance í™•ì¸í•˜ì—¬ ì¤‘ìš” ë³€ìˆ˜ íŒŒì•…
- Threshold ìµœì í™”ë¡œ ì„±ëŠ¥ í–¥ìƒ

## Best Practices

### 1. í•™ìŠµ ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ (`/engineer-features`)
- [ ] í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬ (`/balance-data`)
- [ ] Train/Test ë¶„ë¦¬ í™•ì¸
- [ ] íŠ¹ì„± ê°œìˆ˜ ì¼ì¹˜ í™•ì¸

### 2. í•™ìŠµ í›„ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] Confusion Matrix í™•ì¸
- [ ] F1-Score, PR-AUC ê¸°ë¡
- [ ] Feature importance ë¶„ì„
- [ ] ê³¼ì í•© ì—¬ë¶€ í™•ì¸ (Train vs Test ì„±ëŠ¥)

### 3. í”„ë¡œë•ì…˜ ë°°í¬ ì „
- [ ] ëª¨ë¸ íŒŒì¼ ì €ì¥ í™•ì¸
- [ ] ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì €ì¥ í™•ì¸
- [ ] Threshold ìµœì í™”
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê¸°ë¡

## Next Steps

ëª¨ë¸ í•™ìŠµ í›„ ê¶Œì¥ ë‹¨ê³„:

1. **Feature Importance ë¶„ì„**
   - ì¤‘ìš” ë³€ìˆ˜ Top 20 íŒŒì•…
   - ë¶ˆí•„ìš”í•œ ë³€ìˆ˜ ì œê±°

2. **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹** (ì˜ˆì •)
   - Optuna, GridSearch
   - F1-Score ìµœì í™”

3. **Ensemble** (ì˜ˆì •)
   - XGBoost + LightGBM + RF
   - Voting, Stacking

4. **SHAP ë¶„ì„** (ì˜ˆì •)
   - ì˜ˆì¸¡ ì„¤ëª…
   - ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ
