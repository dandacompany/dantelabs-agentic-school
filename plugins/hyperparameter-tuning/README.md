# Hyperparameter Tuning Plugin

Optunaë¥¼ ì‚¬ìš©í•œ ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” í”ŒëŸ¬ê·¸ì¸ì…ë‹ˆë‹¤.

## ğŸ“‹ ê°œìš”

ì´ í”ŒëŸ¬ê·¸ì¸ì€ ë² ì´ì§€ì•ˆ ìµœì í™”ë¥¼ ì‚¬ìš©í•˜ì—¬ ìë™ìœ¼ë¡œ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ìŠµë‹ˆë‹¤:

- âœ… **Optuna í”„ë ˆì„ì›Œí¬**: TPE Sampler + Median Pruner
- âœ… **ì•Œê³ ë¦¬ì¦˜ ì§€ì›**: XGBoost, LightGBM, Random Forest
- âœ… **ìµœì í™” ì§€í‘œ**: F1-Score, ROC-AUC, PR-AUC
- âœ… **êµì°¨ ê²€ì¦**: Stratified 5-Fold CV
- âœ… **ì¡°ê¸° ì¢…ë£Œ**: ì„±ëŠ¥ ë‚®ì€ ì‹œë„ ìë™ ì¤‘ë‹¨

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. ì˜ì¡´ì„± ì„¤ì¹˜

```bash
cd plugins/hyperparameter-tuning/skills/tuning
uv pip install --system -r requirements.txt
```

### 2. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

```bash
python scripts/tune_model.py \
  --X-train-path "projects/creditcard-fraud-detection/data/processed/X_train_balanced.csv" \
  --y-train-path "projects/creditcard-fraud-detection/data/processed/y_train_balanced.csv" \
  --algorithm xgboost \
  --metric f1 \
  --n-trials 50
```

## ğŸ“ í”ŒëŸ¬ê·¸ì¸ êµ¬ì¡°

```
plugins/hyperparameter-tuning/
â”œâ”€â”€ plugin.json
â”œâ”€â”€ README.md
â”œâ”€â”€ commands/
â”‚   â””â”€â”€ tune-hyperparameters.md
â””â”€â”€ skills/
    â””â”€â”€ tuning/
        â”œâ”€â”€ requirements.txt
        â””â”€â”€ scripts/
            â””â”€â”€ tune_model.py
```

## ğŸ¯ ì£¼ìš” ê¸°ëŠ¥

### 1. Optuna ìµœì í™”
- **TPE Sampler**: íš¨ìœ¨ì ì¸ ë² ì´ì§€ì•ˆ ìµœì í™”
- **Median Pruner**: ì„±ëŠ¥ ë‚®ì€ ì‹œë„ ì¡°ê¸° ì¢…ë£Œ
- **Random Searchë³´ë‹¤ 10-100ë°° ë¹ ë¦„**

### 2. ì§€ì› ì•Œê³ ë¦¬ì¦˜

| ì•Œê³ ë¦¬ì¦˜ | íŠœë‹ íŒŒë¼ë¯¸í„° ìˆ˜ | ê¶Œì¥ trials |
|---------|---------------|------------|
| **XGBoost** | 8ê°œ | 50-100 |
| **LightGBM** | 9ê°œ | 50-100 |
| **Random Forest** | 5ê°œ | 30-50 |

### 3. ìµœì í™” ì§€í‘œ

| ì§€í‘œ | ì‚¬ìš© ì‹œê¸° |
|------|---------|
| **f1** | ë¶ˆê· í˜• ë°ì´í„° (ê¸°ë³¸ ê¶Œì¥) |
| **pr_auc** | ê·¹ì‹¬í•œ ë¶ˆê· í˜• |
| **roc_auc** | ê· í˜• ë°ì´í„° |

## ğŸ“Š ì¶œë ¥

### íŠœë‹ëœ ëª¨ë¸
```
projects/{project-name}/outputs/models/
â”œâ”€â”€ xgboost_tuned_model.pkl         # ìµœì  ëª¨ë¸
â”œâ”€â”€ xgboost_tuning_history.csv      # ìµœì í™” ì´ë ¥
â””â”€â”€ xgboost_best_params.txt         # ìµœì  íŒŒë¼ë¯¸í„°
```

### ìµœì  íŒŒë¼ë¯¸í„° íŒŒì¼
```
Algorithm: xgboost
Metric: f1
Best F1: 0.8567

Best Parameters:
  n_estimators: 150
  max_depth: 6
  learning_rate: 0.0856
  subsample: 0.85
  colsample_bytree: 0.92
  reg_alpha: 0.0023
  reg_lambda: 1.234
  min_child_weight: 3

Optimization Date: 2026-01-31 13:00:00
```

## ğŸ”§ ì‚¬ìš© ì˜ˆì‹œ

### Example 1: ì‹ ìš©ì¹´ë“œ ì‚¬ê¸° íƒì§€ íŠœë‹
```bash
python tune_model.py \
  --X-train-path "projects/creditcard-fraud-detection/data/processed/X_train_balanced.csv" \
  --y-train-path "projects/creditcard-fraud-detection/data/processed/y_train_balanced.csv" \
  --algorithm xgboost \
  --metric pr_auc \
  --n-trials 50
```

**ì˜ˆìƒ ê°œì„ **:
- ê¸°ë³¸ ëª¨ë¸ F1: 0.83
- íŠœë‹ í›„ F1: 0.85-0.87
- **2-4% ì„±ëŠ¥ í–¥ìƒ**

### Example 2: ë¹ ë¥¸ í”„ë¡œí† íƒ€ì… (20 trials, 30ë¶„)
```bash
python tune_model.py \
  --X-train-path "projects/my-project/data/processed/X_train_balanced.csv" \
  --y-train-path "projects/my-project/data/processed/y_train_balanced.csv" \
  --n-trials 20 \
  --timeout 1800
```

### Example 3: ì •ë°€ íŠœë‹ (100 trials, 3ì‹œê°„)
```bash
python tune_model.py \
  --X-train-path "projects/my-project/data/processed/X_train_balanced.csv" \
  --y-train-path "projects/my-project/data/processed/y_train_balanced.csv" \
  --algorithm lightgbm \
  --n-trials 100 \
  --metric f1
```

## ğŸ“ˆ n-trials ê°€ì´ë“œ

| ë°ì´í„° í¬ê¸° | ê¶Œì¥ trials | ì˜ˆìƒ ì‹œê°„ (XGBoost) |
|-----------|------------|-------------------|
| < 10,000ê±´ | 100 | ~30ë¶„ |
| 10K - 100K | 50 | ~1ì‹œê°„ |
| 100K - 1M | 30 | ~2ì‹œê°„ |
| > 1M | 20 | ~3ì‹œê°„ |

## ğŸ¨ ìµœì í™” ì´ë ¥ ì‹œê°í™”

### Python ì˜ˆì œ
```python
import pandas as pd
import matplotlib.pyplot as plt

# ì´ë ¥ ë¡œë“œ
df = pd.read_csv('projects/my-project/outputs/models/xgboost_tuning_history.csv')

# ìµœì í™” ì§„í–‰ ê³¼ì •
plt.figure(figsize=(10, 6))
plt.plot(df['number'], df['value'], marker='o')
plt.xlabel('Trial Number')
plt.ylabel('F1 Score')
plt.title('Hyperparameter Optimization Progress')
plt.grid(True)
plt.savefig('optimization_progress.png')

# ìµœê³  ì„±ëŠ¥ trial
best_trial = df.loc[df['value'].idxmax()]
print(f"Best Trial #{best_trial['number']}: F1 = {best_trial['value']:.4f}")
```

## ğŸ› íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ: ìµœì í™”ê°€ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¼
**í•´ê²°**:
```bash
# timeout ì„¤ì •
--timeout 3600  # 1ì‹œê°„

# trials ì¤„ì´ê¸°
--n-trials 20

# LightGBM ì‚¬ìš© (ë” ë¹ ë¦„)
--algorithm lightgbm
```

### ë¬¸ì œ: ë©”ëª¨ë¦¬ ë¶€ì¡±
**í•´ê²°**:
- ë°ì´í„° ìƒ˜í”Œë§
- LightGBM ì‚¬ìš©
- K-Fold ìˆ˜ ì¤„ì´ê¸° (ìŠ¤í¬ë¦½íŠ¸ ìˆ˜ì •)

### ë¬¸ì œ: ì„±ëŠ¥ ê°œì„  ì—†ìŒ
**í•´ê²°**:
- trials ëŠ˜ë¦¬ê¸° (50 â†’ 100)
- ë‹¤ë¥¸ metric ì‹œë„
- ë°ì´í„° ì „ì²˜ë¦¬ ì¬í™•ì¸

## ğŸ”— ê´€ë ¨ í”ŒëŸ¬ê·¸ì¸

- `model-selection`: ê¸°ë³¸ ëª¨ë¸ í•™ìŠµ
- `imbalance-handling`: í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬ (íŠœë‹ ì „)
- `feature-engineering`: íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ (íŠœë‹ ì „)

## ğŸ“ ë¼ì´ì„ ìŠ¤

MIT License

## ğŸ‘¤ ì‘ì„±ì

- **Dante Labs**
- Email: datapod.k@gmail.com
- ë²„ì „: 1.0.0

## ğŸ’¡ Best Practices

### íŠœë‹ ì „ ì¤€ë¹„
- [ ] ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ
- [ ] í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬
- [ ] ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ ì„±ëŠ¥ ê¸°ë¡

### íŠœë‹ ì „ëµ
- [ ] í”„ë¡œí† íƒ€ì…: 20 trials, 30ë¶„
- [ ] ì¼ë°˜: 50 trials, 1-2ì‹œê°„
- [ ] í”„ë¡œë•ì…˜: 100 trials, 3-4ì‹œê°„

### íŠœë‹ í›„ ê²€ì¦
- [ ] ìµœì  íŒŒë¼ë¯¸í„° í™•ì¸
- [ ] Test ë°ì´í„°ë¡œ ê²€ì¦
- [ ] ê³¼ì í•© ì—¬ë¶€ í™•ì¸
- [ ] ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„ ê°œì„ ìœ¨ ê¸°ë¡
