---
name: tune-hyperparameters
description: Optunaë¥¼ ì‚¬ìš©í•˜ì—¬ ìë™ìœ¼ë¡œ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
arguments:
  - name: X-train-path
    description: Train íŠ¹ì„± ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    required: true
  - name: y-train-path
    description: Train íƒ€ê²Ÿ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    required: true
  - name: algorithm
    description: íŠœë‹í•  ì•Œê³ ë¦¬ì¦˜ (xgboost, lightgbm, random_forest)
    required: false
    default: "xgboost"
  - name: metric
    description: ìµœì í™” ì§€í‘œ (f1, roc_auc, pr_auc)
    required: false
    default: "f1"
  - name: n-trials
    description: ìµœì í™” ì‹œë„ íšŸìˆ˜
    required: false
    default: "50"
  - name: timeout
    description: ìµœì í™” ì œí•œ ì‹œê°„ (ì´ˆ)
    required: false
  - name: output-dir
    description: ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬
    required: false
    default: "projects/{project-name}/outputs/models"
---

# /tune-hyperparameters

Optunaë¥¼ ì‚¬ìš©í•˜ì—¬ ìë™ìœ¼ë¡œ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ê³  ìµœê³  ì„±ëŠ¥ì˜ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.

## Usage

```bash
# XGBoost íŠœë‹ (ê¸°ë³¸ê°’)
/tune-hyperparameters \
  --X-train-path "projects/creditcard-fraud-detection/data/processed/X_train_balanced.csv" \
  --y-train-path "projects/creditcard-fraud-detection/data/processed/y_train_balanced.csv"

# LightGBM íŠœë‹ with PR-AUC
/tune-hyperparameters \
  --X-train-path "projects/my-project/data/processed/X_train_balanced.csv" \
  --y-train-path "projects/my-project/data/processed/y_train_balanced.csv" \
  --algorithm lightgbm \
  --metric pr_auc

# 100íšŒ ì‹œë„
/tune-hyperparameters \
  --X-train-path "projects/my-project/data/processed/X_train_balanced.csv" \
  --y-train-path "projects/my-project/data/processed/y_train_balanced.csv" \
  --algorithm xgboost \
  --n-trials 100

# ì‹œê°„ ì œí•œ (3600ì´ˆ = 1ì‹œê°„)
/tune-hyperparameters \
  --X-train-path "projects/my-project/data/processed/X_train_balanced.csv" \
  --y-train-path "projects/my-project/data/processed/y_train_balanced.csv" \
  --timeout 3600
```

## What This Command Does

### 1. Optuna ìµœì í™” í”„ë ˆì„ì›Œí¬
- **TPE Sampler**: Tree-structured Parzen Estimator (íš¨ìœ¨ì  íƒìƒ‰)
- **Median Pruner**: ì„±ëŠ¥ ë‚®ì€ ì‹œë„ ì¡°ê¸° ì¢…ë£Œ
- **Stratified K-Fold CV**: 5-Fold êµì°¨ ê²€ì¦

### 2. ìµœì í™” ëŒ€ìƒ í•˜ì´í¼íŒŒë¼ë¯¸í„°

#### XGBoost
```python
{
    'n_estimators': [50, 300],        # íŠ¸ë¦¬ ê°œìˆ˜
    'max_depth': [3, 10],             # íŠ¸ë¦¬ ê¹Šì´
    'learning_rate': [0.01, 0.3],     # í•™ìŠµë¥ 
    'subsample': [0.6, 1.0],          # ìƒ˜í”Œ ë¹„ìœ¨
    'colsample_bytree': [0.6, 1.0],   # íŠ¹ì„± ë¹„ìœ¨
    'reg_alpha': [1e-8, 10.0],        # L1 ì •ê·œí™”
    'reg_lambda': [1e-8, 10.0],       # L2 ì •ê·œí™”
    'min_child_weight': [1, 10]       # ìµœì†Œ ìƒ˜í”Œ ê°€ì¤‘ì¹˜
}
```

#### LightGBM
```python
{
    'n_estimators': [50, 300],
    'max_depth': [3, 10],
    'learning_rate': [0.01, 0.3],
    'num_leaves': [20, 100],          # ë¦¬í”„ ê°œìˆ˜
    'subsample': [0.6, 1.0],
    'colsample_bytree': [0.6, 1.0],
    'reg_alpha': [1e-8, 10.0],
    'reg_lambda': [1e-8, 10.0],
    'min_child_samples': [5, 50]      # ìµœì†Œ ìƒ˜í”Œ ìˆ˜
}
```

#### Random Forest
```python
{
    'n_estimators': [50, 300],
    'max_depth': [5, 30],
    'min_samples_split': [2, 20],     # ë¶„í•  ìµœì†Œ ìƒ˜í”Œ
    'min_samples_leaf': [1, 10],      # ë¦¬í”„ ìµœì†Œ ìƒ˜í”Œ
    'max_features': ['sqrt', 'log2', None]
}
```

### 3. ìµœì í™” ì§€í‘œ

| ì§€í‘œ | ì‚¬ìš© ì‹œê¸° | ì„¤ëª… |
|------|---------|------|
| **f1** | ë¶ˆê· í˜• ë°ì´í„° (ê¸°ë³¸ ê¶Œì¥) | Precision-Recall ê· í˜• |
| **pr_auc** | ê·¹ì‹¬í•œ ë¶ˆê· í˜• | PR ê³¡ì„  ì•„ë˜ ë©´ì  |
| **roc_auc** | ê· í˜• ë°ì´í„° | ROC ê³¡ì„  ì•„ë˜ ë©´ì  |

### 4. ì¶œë ¥

#### íŠœë‹ëœ ëª¨ë¸
```
projects/{project-name}/outputs/models/
â”œâ”€â”€ xgboost_tuned_model.pkl         # ìµœì  ëª¨ë¸
â”œâ”€â”€ xgboost_tuning_history.csv      # ìµœì í™” ì´ë ¥
â””â”€â”€ xgboost_best_params.txt         # ìµœì  íŒŒë¼ë¯¸í„°
```

#### ì½˜ì†” ì¶œë ¥
```
============================================================
í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘
============================================================

ë°ì´í„° ë¡œë“œ ì¤‘...
âœ“ Train: 250,196ê±´ Ã— 33ê°œ íŠ¹ì„±

ìµœì í™” ì‹œì‘ (ì•Œê³ ë¦¬ì¦˜: xgboost, ì§€í‘œ: f1)
ì‹œë„ íšŸìˆ˜: 50

[I 2026-01-31 12:00:00,000] Trial 0 finished with value: 0.8156
[I 2026-01-31 12:01:15,000] Trial 1 finished with value: 0.8234
[I 2026-01-31 12:02:30,000] Trial 2 finished with value: 0.8312
...
[I 2026-01-31 13:00:00,000] Trial 49 finished with value: 0.8567

============================================================
ìµœì í™” ì™„ë£Œ
============================================================

ìµœê³  F1: 0.8567

ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:
  n_estimators: 150
  max_depth: 6
  learning_rate: 0.0856
  subsample: 0.85
  colsample_bytree: 0.92
  reg_alpha: 0.0023
  reg_lambda: 1.234
  min_child_weight: 3

ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ ì¤‘...
âœ“ í•™ìŠµ ì™„ë£Œ

âœ“ ëª¨ë¸ ì €ì¥: projects/creditcard-fraud-detection/outputs/models/xgboost_tuned_model.pkl
âœ“ ìµœì í™” ì´ë ¥ ì €ì¥: projects/creditcard-fraud-detection/outputs/models/xgboost_tuning_history.csv
âœ“ ìµœì  íŒŒë¼ë¯¸í„° ì €ì¥: projects/creditcard-fraud-detection/outputs/models/xgboost_best_params.txt

============================================================
í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì™„ë£Œ
============================================================

ğŸ“Š ìµœê³  ì„±ëŠ¥: F1 = 0.8567
ğŸ“ ëª¨ë¸: projects/creditcard-fraud-detection/outputs/models/xgboost_tuned_model.pkl
ğŸ“ ì´ë ¥: projects/creditcard-fraud-detection/outputs/models/xgboost_tuning_history.csv
ğŸ“ íŒŒë¼ë¯¸í„°: projects/creditcard-fraud-detection/outputs/models/xgboost_best_params.txt
```

## Optimization Strategy

### TPE (Tree-structured Parzen Estimator)
- ë² ì´ì§€ì•ˆ ìµœì í™” ê¸°ë°˜
- ì´ì „ ì‹œë„ ê²°ê³¼ë¥¼ í•™ìŠµí•˜ì—¬ íš¨ìœ¨ì  íƒìƒ‰
- Random Searchë³´ë‹¤ 10-100ë°° ë¹ ë¦„

### Median Pruning
- ì„±ëŠ¥ ë‚®ì€ ì‹œë„ ì¡°ê¸° ì¢…ë£Œ
- ê³„ì‚° ìì› ì ˆì•½
- ë¹ ë¥¸ ìˆ˜ë ´

### 5-Fold Cross Validation
- ê³¼ì í•© ë°©ì§€
- ì•ˆì •ì ì¸ ì„±ëŠ¥ ì¶”ì •
- Stratifiedë¡œ í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€

## Performance Tips

### n-trials ì„¤ì • ê°€ì´ë“œ

| ë°ì´í„° í¬ê¸° | ê¶Œì¥ trials | ì˜ˆìƒ ì‹œê°„ (XGBoost) |
|-----------|------------|-------------------|
| < 10,000ê±´ | 100 | ~30ë¶„ |
| 10K - 100K | 50 | ~1ì‹œê°„ |
| 100K - 1M | 30 | ~2ì‹œê°„ |
| > 1M | 20 | ~3ì‹œê°„ |

### ë¹ ë¥¸ íŠœë‹ (í”„ë¡œí† íƒ€ì…)
```bash
/tune-hyperparameters \
  --X-train-path "..." \
  --y-train-path "..." \
  --n-trials 20 \
  --timeout 1800  # 30ë¶„
```

### ì •ë°€ íŠœë‹ (í”„ë¡œë•ì…˜)
```bash
/tune-hyperparameters \
  --X-train-path "..." \
  --y-train-path "..." \
  --n-trials 100 \
  --metric pr_auc
```

## Examples

### Example 1: ì‹ ìš©ì¹´ë“œ ì‚¬ê¸° íƒì§€
```bash
/tune-hyperparameters \
  --X-train-path "projects/creditcard-fraud-detection/data/processed/X_train_balanced.csv" \
  --y-train-path "projects/creditcard-fraud-detection/data/processed/y_train_balanced.csv" \
  --algorithm xgboost \
  --metric pr_auc \
  --n-trials 50
```

**ì˜ˆìƒ ê°œì„ **:
- ê¸°ë³¸ ëª¨ë¸ F1: 0.83 â†’ íŠœë‹ í›„: 0.85-0.87
- ì•½ 2-4% ì„±ëŠ¥ í–¥ìƒ

### Example 2: LightGBM ë¹ ë¥¸ íŠœë‹
```bash
/tune-hyperparameters \
  --X-train-path "projects/my-project/data/processed/X_train_balanced.csv" \
  --y-train-path "projects/my-project/data/processed/y_train_balanced.csv" \
  --algorithm lightgbm \
  --n-trials 30 \
  --timeout 3600
```

### Example 3: Random Forest ì •ë°€ íŠœë‹
```bash
/tune-hyperparameters \
  --X-train-path "projects/my-project/data/processed/X_train_balanced.csv" \
  --y-train-path "projects/my-project/data/processed/y_train_balanced.csv" \
  --algorithm random_forest \
  --metric f1 \
  --n-trials 100
```

## Tuning History Analysis

### CSV íŒŒì¼ êµ¬ì¡°
```csv
number,value,datetime_start,datetime_complete,duration,params_n_estimators,params_max_depth,...
0,0.8156,2026-01-31 12:00:00,2026-01-31 12:01:15,75.2,100,6,...
1,0.8234,2026-01-31 12:01:15,2026-01-31 12:02:30,75.1,150,5,...
...
```

### ì‹œê°í™” ì˜ˆì œ
```python
import pandas as pd
import matplotlib.pyplot as plt

# ì´ë ¥ ë¡œë“œ
df = pd.read_csv('xgboost_tuning_history.csv')

# ìµœì í™” ì§„í–‰ ê³¼ì •
plt.figure(figsize=(10, 6))
plt.plot(df['number'], df['value'])
plt.xlabel('Trial')
plt.ylabel('F1 Score')
plt.title('Hyperparameter Optimization Progress')
plt.savefig('optimization_progress.png')
```

## Troubleshooting

### ë¬¸ì œ: ìµœì í™”ê°€ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¼
**í•´ê²°**:
- `--timeout` ì„¤ì • (ì˜ˆ: 3600ì´ˆ)
- `--n-trials` ì¤„ì´ê¸° (50 â†’ 20)
- LightGBM ì‚¬ìš© (XGBoostë³´ë‹¤ ë¹ ë¦„)

### ë¬¸ì œ: ë©”ëª¨ë¦¬ ë¶€ì¡±
**í•´ê²°**:
- K-Fold ìˆ˜ ì¤„ì´ê¸° (ìŠ¤í¬ë¦½íŠ¸ ìˆ˜ì •: 5 â†’ 3)
- ë°ì´í„° ìƒ˜í”Œë§
- LightGBM ì‚¬ìš©

### ë¬¸ì œ: ì„±ëŠ¥ ê°œì„  ì—†ìŒ
**í•´ê²°**:
- `--n-trials` ëŠ˜ë¦¬ê¸° (50 â†’ 100)
- ë‹¤ë¥¸ `--metric` ì‹œë„
- ë°ì´í„° ì „ì²˜ë¦¬ ì¬í™•ì¸

## Related Commands

- `/train-model`: ê¸°ë³¸ ëª¨ë¸ í•™ìŠµ
- `/balance-data`: í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬ (íŠœë‹ ì „)
- `/engineer-features`: íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ (íŠœë‹ ì „)

## Agents Used

- `hyperparameter-tuner` (í•„ìˆ˜): Optuna ê¸°ë°˜ ìë™ ìµœì í™”

## Notes

âš ï¸ **ì£¼ì˜ì‚¬í•­**:
- íŠœë‹ì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼ (1-3ì‹œê°„)
- ì¶©ë¶„í•œ ë©”ëª¨ë¦¬ í™•ë³´ í•„ìš”
- íŠœë‹ ì „ `/balance-data` í•„ìˆ˜

ğŸ’¡ **íŒ**:
- í”„ë¡œí† íƒ€ì…: 20 trials, 30ë¶„
- í”„ë¡œë•ì…˜: 50-100 trials, 1-3ì‹œê°„
- PR-AUCë¡œ ìµœì í™” (ë¶ˆê· í˜• ë°ì´í„°)
- íŠœë‹ ì´ë ¥ CSVë¡œ ë¶„ì„

## Best Practices

### 1. íŠœë‹ ì „ ì¤€ë¹„
- [ ] ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ
- [ ] í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬
- [ ] ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ ì„±ëŠ¥ í™•ì¸

### 2. íŠœë‹ ì¤‘
- [ ] n-trials ì ì ˆíˆ ì„¤ì • (20-100)
- [ ] timeout ì„¤ì • (ê³¼ë„í•œ ì‹œê°„ ë°©ì§€)
- [ ] ì ì ˆí•œ metric ì„ íƒ

### 3. íŠœë‹ í›„
- [ ] ìµœì  íŒŒë¼ë¯¸í„° í™•ì¸
- [ ] íŠœë‹ ì´ë ¥ ë¶„ì„
- [ ] Test ë°ì´í„°ë¡œ ìµœì¢… ê²€ì¦
- [ ] ê³¼ì í•© ì—¬ë¶€ í™•ì¸
