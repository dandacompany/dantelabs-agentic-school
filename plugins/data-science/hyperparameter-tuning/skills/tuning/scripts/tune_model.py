#!/usr/bin/env python3
"""
í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ìŠ¤í¬ë¦½íŠ¸

Optunaë¥¼ ì‚¬ìš©í•˜ì—¬ ìë™ìœ¼ë¡œ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ìŠµë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python tune_model.py \
      --X-train-path "./data/processed/X_train_balanced.csv" \
      --y-train-path "./data/processed/y_train_balanced.csv" \
      --algorithm xgboost \
      --n-trials 50 \
      --metric f1
"""

import argparse
import joblib
from datetime import datetime
from pathlib import Path

import numpy as np
import pandas as pd
import optuna
from optuna.integration import XGBoostPruningCallback, LightGBMPruningCallback
import xgboost as xgb
import lightgbm as lgb
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import (
    f1_score,
    roc_auc_score,
    precision_recall_curve,
    auc as auc_score
)


def load_data(X_train_path, y_train_path):
    """ë°ì´í„° ë¡œë“œ"""
    print(f"\në°ì´í„° ë¡œë“œ ì¤‘...")
    X_train = pd.read_csv(X_train_path)
    y_train = pd.read_csv(y_train_path).iloc[:, 0]

    print(f"âœ“ Train: {len(X_train):,}ê±´ Ã— {X_train.shape[1]}ê°œ íŠ¹ì„±")

    return X_train, y_train


def objective_xgboost(trial, X, y, metric='f1'):
    """XGBoost ëª©ì  í•¨ìˆ˜"""
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 50, 300),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),
        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),
        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
        'random_state': 42,
        'eval_metric': 'logloss'
    }

    # Stratified K-Fold CV
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    scores = []

    for train_idx, val_idx in cv.split(X, y):
        X_train_cv, X_val_cv = X.iloc[train_idx], X.iloc[val_idx]
        y_train_cv, y_val_cv = y.iloc[train_idx], y.iloc[val_idx]

        model = xgb.XGBClassifier(**params)
        model.fit(
            X_train_cv, y_train_cv,
            eval_set=[(X_val_cv, y_val_cv)],
            verbose=False,
            callbacks=[XGBoostPruningCallback(trial, 'validation_0-logloss')]
        )

        # í‰ê°€
        if metric == 'f1':
            y_pred = model.predict(X_val_cv)
            score = f1_score(y_val_cv, y_pred)
        elif metric == 'roc_auc':
            y_proba = model.predict_proba(X_val_cv)[:, 1]
            score = roc_auc_score(y_val_cv, y_proba)
        elif metric == 'pr_auc':
            y_proba = model.predict_proba(X_val_cv)[:, 1]
            precision, recall, _ = precision_recall_curve(y_val_cv, y_proba)
            score = auc_score(recall, precision)

        scores.append(score)

    return np.mean(scores)


def objective_lightgbm(trial, X, y, metric='f1'):
    """LightGBM ëª©ì  í•¨ìˆ˜"""
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 50, 300),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
        'num_leaves': trial.suggest_int('num_leaves', 20, 100),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),
        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),
        'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),
        'random_state': 42,
        'verbose': -1
    }

    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    scores = []

    for train_idx, val_idx in cv.split(X, y):
        X_train_cv, X_val_cv = X.iloc[train_idx], X.iloc[val_idx]
        y_train_cv, y_val_cv = y.iloc[train_idx], y.iloc[val_idx]

        model = lgb.LGBMClassifier(**params)
        model.fit(
            X_train_cv, y_train_cv,
            eval_set=[(X_val_cv, y_val_cv)],
            callbacks=[LightGBMPruningCallback(trial, 'binary_logloss')]
        )

        # í‰ê°€
        if metric == 'f1':
            y_pred = model.predict(X_val_cv)
            score = f1_score(y_val_cv, y_pred)
        elif metric == 'roc_auc':
            y_proba = model.predict_proba(X_val_cv)[:, 1]
            score = roc_auc_score(y_val_cv, y_proba)
        elif metric == 'pr_auc':
            y_proba = model.predict_proba(X_val_cv)[:, 1]
            precision, recall, _ = precision_recall_curve(y_val_cv, y_proba)
            score = auc_score(recall, precision)

        scores.append(score)

    return np.mean(scores)


def objective_rf(trial, X, y, metric='f1'):
    """Random Forest ëª©ì  í•¨ìˆ˜"""
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 50, 300),
        'max_depth': trial.suggest_int('max_depth', 5, 30),
        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),
        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),
        'random_state': 42,
        'n_jobs': -1
    }

    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    scores = []

    for train_idx, val_idx in cv.split(X, y):
        X_train_cv, X_val_cv = X.iloc[train_idx], X.iloc[val_idx]
        y_train_cv, y_val_cv = y.iloc[train_idx], y.iloc[val_idx]

        model = RandomForestClassifier(**params)
        model.fit(X_train_cv, y_train_cv)

        # í‰ê°€
        if metric == 'f1':
            y_pred = model.predict(X_val_cv)
            score = f1_score(y_val_cv, y_pred)
        elif metric == 'roc_auc':
            y_proba = model.predict_proba(X_val_cv)[:, 1]
            score = roc_auc_score(y_val_cv, y_proba)
        elif metric == 'pr_auc':
            y_proba = model.predict_proba(X_val_cv)[:, 1]
            precision, recall, _ = precision_recall_curve(y_val_cv, y_proba)
            score = auc_score(recall, precision)

        scores.append(score)

    return np.mean(scores)


def train_best_model(best_params, X, y, algorithm):
    """ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ"""
    print(f"\nìµœì  íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ ì¤‘...")

    if algorithm == 'xgboost':
        model = xgb.XGBClassifier(**best_params)
    elif algorithm == 'lightgbm':
        model = lgb.LGBMClassifier(**best_params)
    elif algorithm == 'random_forest':
        model = RandomForestClassifier(**best_params)

    model.fit(X, y)
    print(f"âœ“ í•™ìŠµ ì™„ë£Œ")

    return model


def main():
    parser = argparse.ArgumentParser(description='í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹')

    parser.add_argument('--X-train-path', type=str, required=True)
    parser.add_argument('--y-train-path', type=str, required=True)
    parser.add_argument('--algorithm', type=str, default='xgboost',
                        choices=['xgboost', 'lightgbm', 'random_forest'])
    parser.add_argument('--metric', type=str, default='f1',
                        choices=['f1', 'roc_auc', 'pr_auc'])
    parser.add_argument('--n-trials', type=int, default=50,
                        help='ìµœì í™” ì‹œë„ íšŸìˆ˜ (ê¸°ë³¸ê°’: 50)')
    parser.add_argument('--timeout', type=int, default=None,
                        help='ìµœì í™” ì œí•œ ì‹œê°„ (ì´ˆ)')
    parser.add_argument('--output-dir', type=str, default='outputs/models')

    args = parser.parse_args()

    print("=" * 60)
    print("í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘")
    print("=" * 60)

    # ë°ì´í„° ë¡œë“œ
    X_train, y_train = load_data(args.X_train_path, args.y_train_path)

    # Optuna Study ìƒì„±
    print(f"\nìµœì í™” ì‹œì‘ (ì•Œê³ ë¦¬ì¦˜: {args.algorithm}, ì§€í‘œ: {args.metric})")
    print(f"ì‹œë„ íšŸìˆ˜: {args.n_trials}")

    study = optuna.create_study(
        direction='maximize',
        sampler=optuna.samplers.TPESampler(seed=42),
        pruner=optuna.pruners.MedianPruner(n_warmup_steps=10)
    )

    # ëª©ì  í•¨ìˆ˜ ì„ íƒ
    if args.algorithm == 'xgboost':
        objective = lambda trial: objective_xgboost(trial, X_train, y_train, args.metric)
    elif args.algorithm == 'lightgbm':
        objective = lambda trial: objective_lightgbm(trial, X_train, y_train, args.metric)
    elif args.algorithm == 'random_forest':
        objective = lambda trial: objective_rf(trial, X_train, y_train, args.metric)

    # ìµœì í™” ì‹¤í–‰
    study.optimize(
        objective,
        n_trials=args.n_trials,
        timeout=args.timeout,
        show_progress_bar=True
    )

    # ìµœì  ê²°ê³¼ ì¶œë ¥
    print(f"\n{'=' * 60}")
    print("ìµœì í™” ì™„ë£Œ")
    print(f"{'=' * 60}")

    print(f"\nìµœê³  {args.metric.upper()}: {study.best_value:.4f}")
    print(f"\nìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
    for key, value in study.best_params.items():
        print(f"  {key}: {value}")

    # ìµœì¢… ëª¨ë¸ í•™ìŠµ
    best_model = train_best_model(study.best_params, X_train, y_train, args.algorithm)

    # ëª¨ë¸ ì €ì¥
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    model_path = output_dir / f"{args.algorithm}_tuned_model.pkl"
    joblib.dump(best_model, model_path)
    print(f"\nâœ“ ëª¨ë¸ ì €ì¥: {model_path}")

    # ìµœì í™” ì´ë ¥ ì €ì¥
    history_path = output_dir / f"{args.algorithm}_tuning_history.csv"
    df_history = study.trials_dataframe()
    df_history.to_csv(history_path, index=False)
    print(f"âœ“ ìµœì í™” ì´ë ¥ ì €ì¥: {history_path}")

    # ìµœì  íŒŒë¼ë¯¸í„° ì €ì¥
    params_path = output_dir / f"{args.algorithm}_best_params.txt"
    with open(params_path, 'w') as f:
        f.write(f"Algorithm: {args.algorithm}\n")
        f.write(f"Metric: {args.metric}\n")
        f.write(f"Best {args.metric.upper()}: {study.best_value:.4f}\n")
        f.write(f"\nBest Parameters:\n")
        for key, value in study.best_params.items():
            f.write(f"  {key}: {value}\n")
        f.write(f"\nOptimization Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    print(f"âœ“ ìµœì  íŒŒë¼ë¯¸í„° ì €ì¥: {params_path}")

    print(f"\n{'=' * 60}")
    print("í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì™„ë£Œ")
    print(f"{'=' * 60}")
    print(f"\nğŸ“Š ìµœê³  ì„±ëŠ¥: {args.metric.upper()} = {study.best_value:.4f}")
    print(f"ğŸ“ ëª¨ë¸: {model_path}")
    print(f"ğŸ“ ì´ë ¥: {history_path}")
    print(f"ğŸ“ íŒŒë¼ë¯¸í„°: {params_path}\n")


if __name__ == "__main__":
    main()
